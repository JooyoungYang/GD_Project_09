{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d73edc09",
   "metadata": {},
   "source": [
    "# 프로젝트: KoChatGPT 업그레이드 하기\n",
    "\n",
    "아래 제시된 전략 중 하나를 선택하거나 여러 개를 조합하여 여러분만의 custom ChatGPT를 개발해보자. \n",
    "\n",
    "    1. 이전 노드에서 KoChatGPT에서 사용한 데이터셋을 더 정제하기\n",
    "    2. Human Feedback이 반영된 데이터셋 대체; SFT와 RM 모델에 사용한 다양한 benchmark 데이터셋 검토\n",
    "    3. 최선의 디코딩을 위한 하이퍼파라미터 서치\n",
    "    4. 생성된 답변에 대한 주관적인 평가를 보완할 수 있는 정략적인 메트릭 도입\n",
    "    5. 다양한 Instruction Tuning 및 Prompting 기법 적용\n",
    "    6. 더 큰 파라미터 스케일을 가진 모델 사용\n",
    "    7. 더 효율적인 연산을 수행하는 LoRA 적용 또는 새로운 Instruction Tuning 및 reward ranking 알고리즘 도입\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7311f7d7",
   "metadata": {},
   "source": [
    "Reinforcement Learning from Human Feedback(RLHF): 강화 학습의 한 방법론으로, 사람의 피드백을 직접적인 보상으로 사용하여 모델을 학습.\n",
    "\n",
    "  1. 사람(human demonstrator)이 어떤 작업을 수행하는 것을 관찰하거나, 모델의 행동에 대한 피드백을 직접 제공.\n",
    "  2. 사람의 피드백을 바탕으로 '보상 모델'을 구축. \n",
    "  3. 이 보상 모델을 사용하여 강화학습 알고리즘을 통해 모델을 학습\n",
    "  \n",
    "SFT로 초기 학습을 시작하고, RM을 통해 보상을 구하며, PPO를 사용하여 이 보상을 최대화하는 방향으로 모델을 학습시킨다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730c730e",
   "metadata": {},
   "source": [
    "## STEP 0. 환경 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29ca9044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ab1202",
   "metadata": {},
   "source": [
    "## STEP 1. tokenizer 및 model 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c41f9fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"skt/kogpt2-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c32e54",
   "metadata": {},
   "source": [
    "### tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe744ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gpt2': 1024,\n",
       " 'gpt2-medium': 1024,\n",
       " 'gpt2-large': 1024,\n",
       " 'gpt2-xl': 1024,\n",
       " 'distilgpt2': 1024}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#모델별 최대 입력 크기\n",
    "tokenizer.max_model_input_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af34b35f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>kogpt-2_tokens</th>\n",
       "      <td>▁바람</td>\n",
       "      <td>도</td>\n",
       "      <td>▁없는</td>\n",
       "      <td>▁공중에</td>\n",
       "      <td>▁수직</td>\n",
       "      <td>의</td>\n",
       "      <td>▁파</td>\n",
       "      <td>문을</td>\n",
       "      <td>▁내</td>\n",
       "      <td>이며</td>\n",
       "      <td>▁고</td>\n",
       "      <td>요</td>\n",
       "      <td>히</td>\n",
       "      <td>▁떨어지는</td>\n",
       "      <td>▁오동</td>\n",
       "      <td>잎은</td>\n",
       "      <td>▁누</td>\n",
       "      <td>구의</td>\n",
       "      <td>▁발자</td>\n",
       "      <td>취</td>\n",
       "      <td>▁입</td>\n",
       "      <td>니까</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Input_IDs</th>\n",
       "      <td>10891</td>\n",
       "      <td>7235</td>\n",
       "      <td>9712</td>\n",
       "      <td>49207</td>\n",
       "      <td>14438</td>\n",
       "      <td>8143</td>\n",
       "      <td>9203</td>\n",
       "      <td>9941</td>\n",
       "      <td>9094</td>\n",
       "      <td>9639</td>\n",
       "      <td>9065</td>\n",
       "      <td>8084</td>\n",
       "      <td>8811</td>\n",
       "      <td>21215</td>\n",
       "      <td>34769</td>\n",
       "      <td>19985</td>\n",
       "      <td>9669</td>\n",
       "      <td>10139</td>\n",
       "      <td>21626</td>\n",
       "      <td>8408</td>\n",
       "      <td>9241</td>\n",
       "      <td>23775</td>\n",
       "      <td>389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0     1     2      3      4     5     6     7     8     9   \\\n",
       "kogpt-2_tokens    ▁바람     도   ▁없는   ▁공중에    ▁수직     의    ▁파    문을    ▁내    이며   \n",
       "Input_IDs       10891  7235  9712  49207  14438  8143  9203  9941  9094  9639   \n",
       "\n",
       "                  10    11    12     13     14     15    16     17     18  \\\n",
       "kogpt-2_tokens    ▁고     요     히  ▁떨어지는    ▁오동     잎은    ▁누     구의    ▁발자   \n",
       "Input_IDs       9065  8084  8811  21215  34769  19985  9669  10139  21626   \n",
       "\n",
       "                  19    20     21   22  \n",
       "kogpt-2_tokens     취    ▁입     니까    .  \n",
       "Input_IDs       8408  9241  23775  389  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토크나이저 테스트\n",
    "input_txt = \"바람도 없는 공중에 수직의 파문을 내이며 고요히 떨어지는 오동잎은 누구의 발자취 입니까.\"\n",
    "\n",
    "tokens = tokenizer(input_txt).tokens() #토큰화\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].numpy()  #각 토큰의 ID\n",
    "\n",
    "#토큰과 해당 ID를 매칭\n",
    "pd.options.display.max_columns = 40\n",
    "pd.options.display.max_rows = 60\n",
    "df = pd.DataFrame([tokens, input_ids[0]], index=[\"kogpt-2_tokens\", \"Input_IDs\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0723a22",
   "metadata": {},
   "source": [
    "### decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16233e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "바람도 없는 공중에 수직의 파문을 내이며 고요히 떨어지는 오동잎은 누구의 발자취 입니까.'\n",
      "\"그렇지 않습니다.\"\n",
      "\"어떻게 된 일입니까?\"\n",
      "그녀는 고개를 갸웃거렸다.\n",
      "\"아니, 그게 무슨 말씀이신지 모르겠습니다만.\"\n",
      "\"무슨 말씀인지 알 수가 없군요.\"\n",
      "아무런 대답도 하지 않은 채 그녀는 고개를 끄덕였다.\n",
      "\"그래, 알았어.\"\n",
      "그녀의 눈에서 눈물이 주르륵 흘러내렸다.\n",
      "그녀가 다시 입을 열었다.\n",
      "\"정말 죄송합니다, 고마워요, 고맙습니다\"\n",
      "\"\n"
     ]
    }
   ],
   "source": [
    "max_length=128\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "\n",
    "#Beam Search Decooding\n",
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=10, no_repeat_ngram_size=2, do_sample=False)\n",
    "print(tokenizer.decode(output_beam[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af215231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "바람도 없는 공중에 수직의 파문을 내이며 고요히 떨어지는 오동잎은 누구의 발자취 입니까.\"\n",
      "\"하하, 너는 어디에 있니? 네가 어디로 간다고 말했단 말야?\"\n",
      "눈물이 고여 울고 있었다.\n",
      "\"누구, 어디로 갔나? 너의 이름을 알지도 못하면서 네 이름만 댔으니.\"\n",
      "\"네 말이 맞다. 하지만 나는 너한테 그런 말을 해본 적이 없단 말이야. 그저 네 이름을 잘 기억 못 할 뿐이라고. 내가 알고 있는 거라고는 내가 아니라 너밖에 없다고. 그래서 그렇게 말한 것 아니냐고. 그게 뭔지는 모르겠지만\n"
     ]
    }
   ],
   "source": [
    "# 샘플링 기법 추가 - top_k\n",
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=7, no_repeat_ngram_size=2, do_sample=True, temperature=2.0, top_k=50)\n",
    "print(tokenizer.decode(output_beam[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d82130f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "바람도 없는 공중에 수직의 파문을 내이며 고요히 떨어지는 오동잎은 누구의 발자취 입니까.'\n",
      "\"그렇겠지요.\"\n",
      "\"아니오. 그게 무슨 소리요?\"\n",
      "그녀는 고개를 끄덕였다.\n",
      "\"어떻게 된 일입니까, 선생님? 그건 그렇고.\"\n",
      "\"선생님, 저는 잘 모르겠습니다.\"\n",
      "선생님은 한숨을 내쉬었다.\n",
      "\"그래서 선생님을 만나기로 했어요. 선생님이 저를 만나고 싶다고 하셨거든요. 그런데 선생님은 저에게 아무 말도 하지 않으시더군요. 왜 그러십니까? 저도 선생\n"
     ]
    }
   ],
   "source": [
    "# 샘플링 기법 추가 - top_p\n",
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=7, no_repeat_ngram_size=2, do_sample=True, top_p=0.90)\n",
    "print(tokenizer.decode(output_beam[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af048e8",
   "metadata": {},
   "source": [
    "## STEP 2. EDA 및 데이터 정제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df0c8cd",
   "metadata": {},
   "source": [
    "### Supervised Fine Tuning(SFT) 데이터셋\n",
    "\n",
    "SFT: human demonstrator가 원하는 대답을 제공하면, 이 데이터를 사용하여 미세조정하여 모델을 사전학습 시킴. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caa5bfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 로드\n",
    "import os\n",
    "import json \n",
    "\n",
    "HOME_DIR = os.getenv('HOME')\n",
    "\n",
    "data_path_1_SFT = HOME_DIR + '/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl' \n",
    "with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict= json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83bbc90a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>불고기용 고기 한우에요?</td>\n",
       "      <td>'저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하...</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>쓰던 앱이 유료로 전환됐어</td>\n",
       "      <td>'어떤 앱인지 모르기 때문에 정확한 답변을 드리기 어렵습니다. 하지만, 일반적으로 ...</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>여친이랑 다툼</td>\n",
       "      <td>'저는 인공지능이기 때문에 여친과의 다툼에 대해서는 조언을 드릴 수는 없습니다. 하...</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>술 먹고 싶어</td>\n",
       "      <td>'술은 알코올이 함유된 음료수이며, 건강에 나쁜 영향을 미칠 수 있습니다. 따라서 ...</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>잊고싶다.</td>\n",
       "      <td>'저도 인공지능 엔진으로써 사용자의 개인정보나 감정을 침해할 수 없습니다. 그렇기 ...</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           prompt                                         completion  tokens\n",
       "0   불고기용 고기 한우에요?  '저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하...     193\n",
       "1  쓰던 앱이 유료로 전환됐어  '어떤 앱인지 모르기 때문에 정확한 답변을 드리기 어렵습니다. 하지만, 일반적으로 ...     288\n",
       "2         여친이랑 다툼  '저는 인공지능이기 때문에 여친과의 다툼에 대해서는 조언을 드릴 수는 없습니다. 하...     153\n",
       "3         술 먹고 싶어  '술은 알코올이 함유된 음료수이며, 건강에 나쁜 영향을 미칠 수 있습니다. 따라서 ...     189\n",
       "4           잊고싶다.  '저도 인공지능 엔진으로써 사용자의 개인정보나 감정을 침해할 수 없습니다. 그렇기 ...     147"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#데이터 확인\n",
    "import pandas as pd\n",
    "\n",
    "df_sft = pd.DataFrame(list_data_dict)\n",
    "df_sft.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bf209f",
   "metadata": {},
   "source": [
    "prompt: 사람이 한 질문\n",
    "\n",
    "completion: 모델이 생성한 답변\n",
    "\n",
    "tokens: 총 토큰 수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e9dd631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#데이터('prompt'와 'completion') 길이 분포\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_sft['prompt_length'] = df_sft['prompt'].apply(lambda x: len(x.split()))\n",
    "\n",
    "plt.hist(df_sft['prompt_length'], bins=20, color='blue', alpha=0.5, label='Prompt')\n",
    "plt.xlabel('Prompt Length (Token Count)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Prompt Lengths')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17b9c64a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 토큰 수가 10보다 큰 'prompt' 데이터 제거한 데이터 길이 분포\n",
    "df_sft = df_sft[df_sft['prompt'].apply(lambda x: len(x.split()) <= 10)]\n",
    "\n",
    "df_sft['prompt_length'] = df_sft['prompt'].apply(lambda x: len(x.split()))\n",
    "\n",
    "plt.hist(df_sft['prompt_length'], bins=20, color='blue', alpha=0.5, label='Prompt')\n",
    "plt.xlabel('Prompt Length (Token Count)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Prompt Lengths')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45192dc6",
   "metadata": {},
   "source": [
    "### Reward Model(RM) 데이터셋\n",
    "\n",
    "RM: 사람의 피드백을 바탕으로 구축된 보상 모델\n",
    "\n",
    "여러 출력 후보를 사람에게 제시하고, 그 사람이 어느 것이 더 나은지를 평가한다. \n",
    "\n",
    "이 평가를 통해 모델의 출력에 대한 보상을 생성한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84229d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 로드\n",
    "data_path_2_RM = HOME_DIR + '/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_2_RM.jsonl'\n",
    "with open(data_path_2_RM, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce8b096f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion_0</th>\n",
       "      <th>completion_1</th>\n",
       "      <th>completion_2</th>\n",
       "      <th>ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독...</td>\n",
       "      <td>Allow me to answer your question. I know that ...</td>\n",
       "      <td>번디는 다양한 인터뷰자들과 뉴스홍보 담당자들과의 면담 때 밝혔다.</td>\n",
       "      <td>라이언에게 말했다.</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>개포주공아파트는 몇 단지로 이루어져 있나?</td>\n",
       "      <td>개포주공아파트는 다섯 단지로 이루어져 있습니다.</td>\n",
       "      <td>이날 목송에서 구글상위노</td>\n",
       "      <td>개포주공아파트는 총 27개 단지로 이루어져 있습니다.</td>\n",
       "      <td>[2, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>김영삼의 후보 시절 지역표심을 겨냥한 발언을 문제삼은 후보는?</td>\n",
       "      <td>The diameter of the Metallic domain is bigger ...</td>\n",
       "      <td>이 질문은 조금 불분명합니다. 김영삼 대통령이 후보 시절에 어떤 발언을 했고, 누가...</td>\n",
       "      <td>김영삼의 후보 시절에 지역표심을 겨냥한 발언은 대통령 당선 전까지 대한민국 정부가 ...</td>\n",
       "      <td>[1, 2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>페르시아로부터 기원된 아랍요리의 특징이 뭐야</td>\n",
       "      <td>1. 다양한 스파이스와 허브 사용\\n2. 쌀, 콩, 나물, 고기, 양파 등 다양한 ...</td>\n",
       "      <td>- 아랍요리는 소금, 산 재료, 일부 채소, 고기, 생선 또는 난류의 조합으로 만들...</td>\n",
       "      <td>위한 태야 대표 기원들 개인이라 겪고 태야\\n\\n\\n귀식 걸 공개한 구문\\n\\n사띩...</td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>중화인민공화국의 기본 법률은 누가 제정 및 개정하는가?</td>\n",
       "      <td>중화인민공화국의 기본 법률은 중국인민대표대회(국회)가 제정하고 개정합니다.</td>\n",
       "      <td>The limit of the Office of the Vice President is?</td>\n",
       "      <td>중화인민공화국의 기본 법률은 중화인민공화국의 의회가 제정 및 개정합니다.</td>\n",
       "      <td>[0, 2, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독...   \n",
       "1                            개포주공아파트는 몇 단지로 이루어져 있나?   \n",
       "2                 김영삼의 후보 시절 지역표심을 겨냥한 발언을 문제삼은 후보는?   \n",
       "3                           페르시아로부터 기원된 아랍요리의 특징이 뭐야   \n",
       "4                     중화인민공화국의 기본 법률은 누가 제정 및 개정하는가?   \n",
       "\n",
       "                                        completion_0  \\\n",
       "0  Allow me to answer your question. I know that ...   \n",
       "1                         개포주공아파트는 다섯 단지로 이루어져 있습니다.   \n",
       "2  The diameter of the Metallic domain is bigger ...   \n",
       "3  1. 다양한 스파이스와 허브 사용\\n2. 쌀, 콩, 나물, 고기, 양파 등 다양한 ...   \n",
       "4          중화인민공화국의 기본 법률은 중국인민대표대회(국회)가 제정하고 개정합니다.   \n",
       "\n",
       "                                        completion_1  \\\n",
       "0               번디는 다양한 인터뷰자들과 뉴스홍보 담당자들과의 면담 때 밝혔다.   \n",
       "1                                      이날 목송에서 구글상위노   \n",
       "2  이 질문은 조금 불분명합니다. 김영삼 대통령이 후보 시절에 어떤 발언을 했고, 누가...   \n",
       "3  - 아랍요리는 소금, 산 재료, 일부 채소, 고기, 생선 또는 난류의 조합으로 만들...   \n",
       "4  The limit of the Office of the Vice President is?   \n",
       "\n",
       "                                        completion_2    ranking  \n",
       "0                                         라이언에게 말했다.  [2, 1, 0]  \n",
       "1                      개포주공아파트는 총 27개 단지로 이루어져 있습니다.  [2, 0, 1]  \n",
       "2  김영삼의 후보 시절에 지역표심을 겨냥한 발언은 대통령 당선 전까지 대한민국 정부가 ...  [1, 2, 0]  \n",
       "3  위한 태야 대표 기원들 개인이라 겪고 태야\\n\\n\\n귀식 걸 공개한 구문\\n\\n사띩...  [0, 1, 2]  \n",
       "4           중화인민공화국의 기본 법률은 중화인민공화국의 의회가 제정 및 개정합니다.  [0, 2, 1]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#데이터 확인\n",
    "import pandas as pd\n",
    "\n",
    "df_rm = pd.DataFrame(list_data_dict)\n",
    "df_rm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb83de2",
   "metadata": {},
   "source": [
    "prompt: 사람이 한 질문\n",
    "\n",
    "completion_0, 1, 2: 여러 모델의 답변\n",
    "\n",
    "ranking: 사람이 정한 각 답변의 품질 순위"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6afb4ca",
   "metadata": {},
   "source": [
    "### Proximal Policy Optimization (PPO) 데이터셋\n",
    "\n",
    "PPO: 강화 학습 알고리즘\n",
    "\n",
    "SFT로 미세 조정된 모델을 PPO를 사용하여 추가로 학습시킨다. \n",
    "\n",
    "이때, 위에서 구축된 RM을 사용하여 모델을 학습시킨다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "538e42d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 로드\n",
    "data_path_3_PPO = HOME_DIR + '/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_3_PPO.jsonl'\n",
    "with open(data_path_3_PPO, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict= json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5058f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>개포주공아파트는 몇 단지로 이루어져 있나?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>김영삼의 후보 시절 지역표심을 겨냥한 발언을 문제삼은 후보는?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>페르시아로부터 기원된 아랍요리의 특징이 뭐야</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>중화인민공화국의 기본 법률은 누가 제정 및 개정하는가?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt\n",
       "0  번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독...\n",
       "1                            개포주공아파트는 몇 단지로 이루어져 있나?\n",
       "2                 김영삼의 후보 시절 지역표심을 겨냥한 발언을 문제삼은 후보는?\n",
       "3                           페르시아로부터 기원된 아랍요리의 특징이 뭐야\n",
       "4                     중화인민공화국의 기본 법률은 누가 제정 및 개정하는가?"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#데이터 확인\n",
    "import pandas as pd\n",
    "\n",
    "df_ppo = pd.DataFrame(list_data_dict)\n",
    "df_ppo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751479b1",
   "metadata": {},
   "source": [
    "prompt: 사람이 한 질문"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfc6c9d",
   "metadata": {},
   "source": [
    "## STEP 3-1. SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46bcc0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import Adam\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from copy import deepcopy\n",
    "import copy\n",
    "import logging\n",
    "import json\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cda357",
   "metadata": {},
   "source": [
    "### 모델과 토크나이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80d2c951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2TokenizerFast(name_or_path='skt/kogpt2-base-v2', vocab_size=51200, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=True)\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('skt/kogpt2-base-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained('skt/kogpt2-base-v2',\n",
    "                                          bos_token='</s>',     # 시작 토큰\n",
    "                                          eos_token='</s>',     # 종료 토큰\n",
    "                                          unk_token='</s>',     # OOV 토큰\n",
    "                                          pad_token='</s>',     # 패딩 토큰\n",
    "                                          padding_side=\"right\", # 패딩 방향\n",
    "                                          model_max_length=512, # 입력 문장 최대 길이\n",
    "                                         )\n",
    "\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a005509",
   "metadata": {},
   "source": [
    "### 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aeadbc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Sequence\n",
    "\n",
    "\n",
    "class SFT_dataset(Dataset): \n",
    "    \n",
    "    def __init__(self, \n",
    "                 data_path_1_SFT: str, # SFT 데이터 위치\n",
    "                 tokenizer: transformers.PreTrainedTokenizer,\n",
    "                 verbose=False,\n",
    "                ):\n",
    "        super(SFT_dataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "\n",
    "        # column 이름\n",
    "        pattern_instruction = 'prompt' # instruction\n",
    "        pattern_output = 'completion' # response\n",
    "\n",
    "        with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
    "            list_data_dict = json.load(json_file)\n",
    "\n",
    "        # 지시문\n",
    "        PROMPT_DICT = {\n",
    "            \"prompt_input\": (\n",
    "                \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "            )\n",
    "        }\n",
    "        prompt_input = PROMPT_DICT[\"prompt_input\"]\n",
    "\n",
    "        # 전처리\n",
    "        sources = []\n",
    "        for example in list_data_dict:\n",
    "            tmp = prompt_input.format_map(example) # 지시문 추가\n",
    "            sources.append(tmp)\n",
    "\n",
    "        targets = []\n",
    "        for example in list_data_dict:\n",
    "            targets.append(f\"{example[pattern_output]}{tokenizer.eos_token}\") # 종료 토큰 추가\n",
    "        examples = [s + t for s, t in zip(sources, targets)] # 질문 문장과 대답 문장 연결\n",
    "\n",
    "        # 토큰화\n",
    "        sources_tokenized = self._tokenize_fn(sources, tokenizer)  \n",
    "        examples_tokenized = self._tokenize_fn(examples, tokenizer)  \n",
    "\n",
    "        # 정수화\n",
    "        input_ids = examples_tokenized[\"input_ids\"]\n",
    "        labels = copy.deepcopy(input_ids)\n",
    "        for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "            label[:source_len] = -100 \n",
    "\n",
    "        # 데이터셋 생성\n",
    "        data_dict = dict(input_ids=input_ids, labels=labels)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        logging.warning(\"Loading data done!!: %d\"%(len(self.labels)))\n",
    "\n",
    "\n",
    "    def _tokenize_fn(self, \n",
    "                     strings: Sequence[str], # 토큰화할 문장 리스트\n",
    "                     tokenizer: transformers.PreTrainedTokenizer, \n",
    "                    ) -> Dict:\n",
    "        \"\"\"\n",
    "        문장을 토큰화해 시퀀스 정보를 딕셔너리에 담아 변환\n",
    "        \"\"\"\n",
    "        # 문장 토큰화\n",
    "        tokenized_list = [\n",
    "            tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"longest\",\n",
    "                max_length=tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "            )\n",
    "            for text in strings\n",
    "        ]\n",
    "        \n",
    "        # 정수 시퀀스\n",
    "        input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "        \n",
    "        # 시퀀스 길이\n",
    "        input_ids_lens = labels_lens = [\n",
    "            tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() # pad_token이 아닌 토큰 개수\n",
    "            for tokenized in tokenized_list\n",
    "        ]\n",
    "        \n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            input_ids_lens=input_ids_lens,\n",
    "            labels_lens=labels_lens,\n",
    "        )\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        데이터셋 크기 반환\n",
    "        \"\"\"\n",
    "        return len(self.input_ids)\n",
    "\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        데이터셋의 샘플 반환\n",
    "        \"\"\"\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c316e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention mask 추가\n",
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object): \n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        \n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value= -100)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e7d32a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading data...\n",
      "WARNING:root:Loading data done!!: 12000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input : tensor([  739,   378,   378,   378, 14659, 13394, 37091, 10651,   383, 25841,\n",
      "         8006, 14914,   375,  7673, 20479,  8091, 22311,  9036, 30902, 13675,\n",
      "          375,   378,   378,   378, 41951,   454,  9549, 20549,   383,  8142,\n",
      "         7192, 14914,   382, 37767, 13753,  8263,  7166,   739,  8352,  7659,\n",
      "         9594, 25585, 13600,  8022,  9378, 11532,  9887, 11218,  9111, 16691,\n",
      "        10351, 10561,  9128, 20479,  8091,  9065,  9446,  9036, 28420, 26521,\n",
      "        10163, 26367,  6958,  9030,  9882, 12317, 25882,  9209, 37194, 10351,\n",
      "         9036, 12168, 10529, 15989,  9719, 15434, 10552, 11188, 13362,  9036,\n",
      "        15805, 11300, 11846,  9146, 16691,  9181,  7397, 15806, 13480, 11342,\n",
      "        17596,  9161, 19996,  9025, 25006, 18595,  9966, 12592, 10751, 11814,\n",
      "         8711,  9046, 12450,  9117,  7377, 12521,     1])\n",
      "output: tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,   382, 37767, 13753,  8263,  7166,   739,  8352,  7659,\n",
      "         9594, 25585, 13600,  8022,  9378, 11532,  9887, 11218,  9111, 16691,\n",
      "        10351, 10561,  9128, 20479,  8091,  9065,  9446,  9036, 28420, 26521,\n",
      "        10163, 26367,  6958,  9030,  9882, 12317, 25882,  9209, 37194, 10351,\n",
      "         9036, 12168, 10529, 15989,  9719, 15434, 10552, 11188, 13362,  9036,\n",
      "        15805, 11300, 11846,  9146, 16691,  9181,  7397, 15806, 13480, 11342,\n",
      "        17596,  9161, 19996,  9025, 25006, 18595,  9966, 12592, 10751, 11814,\n",
      "         8711,  9046, 12450,  9117,  7377, 12521,     1])\n"
     ]
    }
   ],
   "source": [
    "SFT_DATASET_PATH = HOME_DIR + '/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl'\n",
    "\n",
    "train_dataset = SFT_dataset(data_path_1_SFT=SFT_DATASET_PATH, tokenizer=tokenizer)\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "\n",
    "print('input : %s'%train_dataset.input_ids[0])\n",
    "print('output: %s'%train_dataset.labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6bc083",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4fded77",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"aiffel/KoChatGPT/test\", #저장 위치\n",
    "    overwrite_output_dir=True, #덮어 쓰기\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=5,#학습률을 0에서 시작하여 원하는 최대 학습률까지 선형적으로 증가\n",
    "    prediction_loss_only=True, #prediction loss만 계산하여 반환\n",
    "    fp16 = True  #16비트 부동 소수점 연산을 사용\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88db4a22",
   "metadata": {},
   "source": [
    "### 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82a601bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 06:04, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.984100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.776800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.687200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "model.save_pretrained('/aiffel/KoChatGPT/output_1_SFT')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c6f283",
   "metadata": {},
   "source": [
    "### 학습 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3868dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):'저는 인공지능 어시스턴트이기 때문에 불고기용 고기의 종류와 양에 대한 정보를 가지고 있지 않습니다. 하지만 일반적으로 불고기는 쇠고기와 함께 먹는 음식 중 하나입니다. 따라서 불고기를 먹을 수 있는 종류는 다양합니다. 예를 들어, 닭가슴살 스테이크, 오므라이스 샐러드 등이 있습니다.\n",
      "\n",
      "### Instruction(명령어):\n",
      "리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "\n",
      "### Response(응답):'리처드 닉슨은 42대 부통령직을 수행했습니다.作)作)은 \"리처드 닉슨\"이 41대 부통령을 수행한 년도를 가리키는 말입니다.作)는 \"리처드 닉슨\"이 40대 부통령을 맡았던 년도를 의미합니다.作은 \"리처드슨\"이 50대 부통령\n",
      "\n",
      "### Instruction(명령어):\n",
      "시카고 오헤어 국제공항은 어디에 있어?\n",
      "\n",
      "### Response(응답):'시카고 오 헤어 국제공항은 미국 캘리포니아주 샌프란시스코에 위치해 있습니다.子供共和國際空港)이라고 불립니다.子供公和国際空港이라는 뜻입니다.子供空和國際公港이라는 이름을 가진 항공사는 다음과 같습니다.\\n\\n1. 대한항공\n",
      "\n",
      "### Instruction(명령어):\n",
      "오늘 미세먼지 어때?\n",
      "\n",
      "### Response(응답):'저는 인공지능 챗봇으로써 미세먼지 정보를 알 수 없습니다. 미세먼지 예보를 확인해 보시는 것이 좋겠습니다.\\n\\n미세먼지 예보: 일반적으로 미세먼지는 주로 중국에서 발원하여 중국 전역으로 퍼져나가기 때문에 중국발 미세먼지가 유입될\n"
     ]
    }
   ],
   "source": [
    "#텍스트 생성을 위한 파이프라인 설정 \n",
    "generator = pipeline('text-generation', model='aiffel/KoChatGPT/output_1_SFT', tokenizer=tokenizer)\n",
    "\n",
    "#텍스트 생성 시 사용될 파라미터 설정\n",
    "generation_args = dict(   \n",
    "    num_beams=4,\n",
    "    repetition_penalty=2.0, #반복적으로 동일한 단어나 구를 생성하는 것을 억제 \n",
    "    no_repeat_ngram_size=4,\n",
    "    eos_token_id=375, # \\n   \n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "#지시문\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "#사용자가 모델에게 던질 질문 리스트\n",
    "list_prompt = ['불고기용 고기 한우에요?',\n",
    "               '리처드 닉슨이 43대 부통령직을 수행한 년도는?',\n",
    "               '시카고 오헤어 국제공항은 어디에 있어?',\n",
    "               '오늘 미세먼지 어때?']\n",
    "\n",
    "#각 질문 앞에 지시문이 추가\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt' : tmp}) for tmp in list_prompt]\n",
    "\n",
    "#각 질문과 그에 대한 응답을 출력\n",
    "list_result = generator(list_prompt, **generation_args)   \n",
    "for prompt, result in zip(list_prompt, list_result):\n",
    "    print()\n",
    "    print((result[0]['generated_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "847e070d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPU 메모리 초기화\n",
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd894578",
   "metadata": {},
   "source": [
    "## STEP 3-2. RM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "edfaab9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cedfe20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install \"$HOME/aiffel/KoChatGPT/colossalai_ChatGPT_230319/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "644dbca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from chatgpt.dataset import RewardDataset\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.trainer import RewardModelTrainer\n",
    "from chatgpt.trainer.strategies import NaiveStrategy\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel, AutoConfig\n",
    "from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
    "import loralib as lora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c61e7d",
   "metadata": {},
   "source": [
    "### 모델 클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0a00539",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTRM_custom(RewardModel):\n",
    "\n",
    "    def __init__(self,\n",
    "                 pretrained: Optional[str] = None,\n",
    "                 config: Optional[GPT2Config] = None,\n",
    "                 checkpoint: bool = False,\n",
    "                 lora_rank: int = 0, #LoRA 적용시 사용되는 rank 수\n",
    "                 lora_train_bias: str = 'none', #LoRA 적용 시 사용되는 학습 bias 방식 적용 \n",
    "                 tokenizer=None) -> None:\n",
    "        if pretrained is not None:\n",
    "            #지정된 pretrained 모델 불러오기\n",
    "            model = GPT2Model.from_pretrained(pretrained)\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "        elif config is not None:\n",
    "            #지정된 config를 적용하여 GPT2 모델 불러오기\n",
    "            model = GPT2Model(config)\n",
    "        else:\n",
    "            #기본 설정의 GPT2 모델 불러오기\n",
    "            model = GPT2Model(GPT2Config())\n",
    "        if checkpoint:\n",
    "            model.gradient_checkpointing_enable()\n",
    "        \n",
    "        #reward가 스칼라값으로 출력되도록 설정\n",
    "        value_head = nn.Linear(model.config.n_embd, 1)\n",
    "        super().__init__(model, value_head, lora_rank, lora_train_bias)\n",
    "        \n",
    "        #모델 멤버 변수에 모델 객체를 저장\n",
    "        if pretrained is not None:\n",
    "            self.model = model\n",
    "            self.pretrained = pretrained\n",
    "\n",
    "\n",
    "    def save_pretrained(self, dir):\n",
    "        if self.pretrained is not None:\n",
    "            self.model.save_pretrained(dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49f860c",
   "metadata": {},
   "source": [
    "### 모델 및 토크나이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e01ca91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at skt/kogpt2-base-v2 were not used when initializing GPT2Model: ['lm_head.weight']\n",
      "- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('skt/kogpt2-base-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=512,\n",
    ")\n",
    "\n",
    "with NaiveStrategy().model_init_context():\n",
    "        model = GPTRM_custom(pretrained='skt/kogpt2-base-v2', lora_rank=0, tokenizer=tokenizer).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfad7e6",
   "metadata": {},
   "source": [
    "### 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb973204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before data num: 10220\n",
      "after  data num: 30660\n",
      "data example: \n",
      "{'prompt': '애플은 리사를 어떻게 처리했어', 'chosen': '애플이 누구인지 명확히 알 수 없어서, 리사가 누구인지와 어떤 상황에서 처리되었는지에 대한 추가적인 정보가 필요합니다. 따라서, 보다 정확한 답변을 제공할 수 없습니다.', 'rejected': '애플은 리사를 위해 고객 서비스 부서에서 고객 다양한 컴퓨터 관련 문제에 대해 응답하는 데 필요한 모든 지원을 제공했습니다. 사용자가 하드웨어 문제를 경험할 때, 전문가들은 필요한 수리(수리, 추가 부품 제공, 소프트웨어 업그레이드 등)을 제공해 드릴 수 있습니다. 또한, 사용자가 사용 방법 문제나 기타 문제를 경험할 때, 대화 상대로 사용자를 지원할 수 있는 전문 고객 서비스 직원들이 사용자에게 상담하고 도움을 주는 데 도움이 될 수 있는 정보를 제공합니다. 또한, 인터넷에서 제공되는 정보를 통해 문제를 해결하거나 고객 서비스 웹 사이트를 통해 자신의 문제를 진단할 수 있도록 하는 등 다양한 방법으로 리사를 처리해 왔습니다.'}\n"
     ]
    }
   ],
   "source": [
    "with open(HOME_DIR + '/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_2_RM.jsonl', \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "total_data_ranking2chosen = []\n",
    "for tmp in list_data_dict:\n",
    "    one_data_ranking2chosen = []\n",
    "\n",
    "    # 랭킹이 높은 답변을 chosen으로 저장\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][1]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][1] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    total_data_ranking2chosen.extend(one_data_ranking2chosen)\n",
    "\n",
    "print('before data num: %d'%(len(list_data_dict)))\n",
    "print('after  data num: %d'%(len(total_data_ranking2chosen)))\n",
    "print('data example: \\n%s'%total_data_ranking2chosen[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab0b59cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': '유아인이 류승완 감독을 만나 영화 베테랑의 시나리오를 받았던 곳은?', 'chosen': '유아인이 류승완 감독을 만나 영화 베테랑의 시나리오를 받았던 곳은 류승완의 사무실입니다.', 'rejected': '대구 영화사옥'}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(230319)\n",
    "random.shuffle(total_data_ranking2chosen)\n",
    "print(total_data_ranking2chosen[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bcc56ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1107.93it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 1015.71it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = total_data_ranking2chosen[:1000] \n",
    "eval_data = total_data_ranking2chosen[1000:1200]\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(eval_data))\n",
    "\n",
    "train_dataset = RewardDataset(train_data, tokenizer, 512)\n",
    "eval_dataset = RewardDataset(eval_data, tokenizer, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e8b2ee49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################################################\n",
      "## prompt ##\n",
      "흑고래의 무게는 어느 정도야\n",
      "######################################################################\n",
      "## chosen ##\n",
      "흑고래의 평균 몸무게는 약 25~40톤 정도이지만, 최대 몸무게는 50톤 이상에 이를 수 있습니다.\n",
      "######################################################################\n",
      "## rejected ##\n",
      "흑고래의 무게는 매우 다양하게 달라집니다. 약 200kg에서 10톤까지 달라질 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 확인 \n",
    "idx = 1\n",
    "print('#'*70)\n",
    "print('## prompt ##')\n",
    "print(train_data[idx]['prompt'])\n",
    "print('#'*70)\n",
    "print('## chosen ##')\n",
    "print(train_data[idx]['chosen'])\n",
    "print('#'*70)\n",
    "print('## rejected ##')\n",
    "print(train_data[idx]['rejected'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f588730a",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fb7e774f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = RewardModelTrainer(model=model,\n",
    "                             strategy=NaiveStrategy(),\n",
    "                             optim=Adam(model.parameters(), lr=5e-5),\n",
    "                             train_dataset=train_dataset,\n",
    "                             eval_dataset=eval_dataset,\n",
    "                             batch_size=4,\n",
    "                             max_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b303001",
   "metadata": {},
   "source": [
    "### 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5243612b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Train step of epoch 0:   0%|          | 0/250 [00:00<?, ?it/s]\u001b[A\n",
      "Train step of epoch 0:   0%|          | 1/250 [00:00<03:38,  1.14it/s]\u001b[A\n",
      "Train step of epoch 0:   0%|          | 1/250 [00:00<03:38,  1.14it/s, loss=0.721]\u001b[A\n",
      "Train step of epoch 0:   1%|          | 2/250 [00:01<03:33,  1.16it/s, loss=0.721]\u001b[A\n",
      "Train step of epoch 0:   1%|          | 2/250 [00:01<03:33,  1.16it/s, loss=0.751]\u001b[A\n",
      "Train step of epoch 0:   1%|          | 3/250 [00:02<03:31,  1.17it/s, loss=0.751]\u001b[A\n",
      "Train step of epoch 0:   1%|          | 3/250 [00:02<03:31,  1.17it/s, loss=0.534]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 4/250 [00:03<03:30,  1.17it/s, loss=0.534]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 4/250 [00:03<03:30,  1.17it/s, loss=0.476]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 5/250 [00:04<03:29,  1.17it/s, loss=0.476]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 5/250 [00:04<03:29,  1.17it/s, loss=0.448]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 6/250 [00:05<03:27,  1.17it/s, loss=0.448]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 6/250 [00:05<03:27,  1.17it/s, loss=1.22] \u001b[A\n",
      "Train step of epoch 0:   3%|▎         | 7/250 [00:05<03:26,  1.17it/s, loss=1.22]\u001b[A\n",
      "Train step of epoch 0:   3%|▎         | 7/250 [00:06<03:26,  1.17it/s, loss=0.467]\u001b[A\n",
      "Train step of epoch 0:   3%|▎         | 8/250 [00:06<03:26,  1.17it/s, loss=0.467]\u001b[A\n",
      "Train step of epoch 0:   3%|▎         | 8/250 [00:06<03:26,  1.17it/s, loss=1.44] \u001b[A\n",
      "Train step of epoch 0:   4%|▎         | 9/250 [00:07<03:25,  1.17it/s, loss=1.44]\u001b[A\n",
      "Train step of epoch 0:   4%|▎         | 9/250 [00:07<03:25,  1.17it/s, loss=0.419]\u001b[A\n",
      "Train step of epoch 0:   4%|▍         | 10/250 [00:08<03:25,  1.17it/s, loss=0.419]\u001b[A\n",
      "Train step of epoch 0:   4%|▍         | 10/250 [00:08<03:25,  1.17it/s, loss=0.508]\u001b[A\n",
      "Train step of epoch 0:   4%|▍         | 11/250 [00:09<03:24,  1.17it/s, loss=0.508]\u001b[A\n",
      "Train step of epoch 0:   4%|▍         | 11/250 [00:09<03:24,  1.17it/s, loss=0.485]\u001b[A\n",
      "Train step of epoch 0:   5%|▍         | 12/250 [00:10<03:23,  1.17it/s, loss=0.485]\u001b[A\n",
      "Train step of epoch 0:   5%|▍         | 12/250 [00:10<03:23,  1.17it/s, loss=0.399]\u001b[A\n",
      "Train step of epoch 0:   5%|▌         | 13/250 [00:11<03:22,  1.17it/s, loss=0.399]\u001b[A\n",
      "Train step of epoch 0:   5%|▌         | 13/250 [00:11<03:22,  1.17it/s, loss=0.153]\u001b[A\n",
      "Train step of epoch 0:   6%|▌         | 14/250 [00:11<03:21,  1.17it/s, loss=0.153]\u001b[A\n",
      "Train step of epoch 0:   6%|▌         | 14/250 [00:11<03:21,  1.17it/s, loss=1.1]  \u001b[A\n",
      "Train step of epoch 0:   6%|▌         | 15/250 [00:12<03:20,  1.17it/s, loss=1.1]\u001b[A\n",
      "Train step of epoch 0:   6%|▌         | 15/250 [00:12<03:20,  1.17it/s, loss=0.567]\u001b[A\n",
      "Train step of epoch 0:   6%|▋         | 16/250 [00:13<03:20,  1.17it/s, loss=0.567]\u001b[A\n",
      "Train step of epoch 0:   6%|▋         | 16/250 [00:13<03:20,  1.17it/s, loss=1.34] \u001b[A\n",
      "Train step of epoch 0:   7%|▋         | 17/250 [00:14<03:19,  1.17it/s, loss=1.34]\u001b[A\n",
      "Train step of epoch 0:   7%|▋         | 17/250 [00:14<03:19,  1.17it/s, loss=0.703]\u001b[A\n",
      "Train step of epoch 0:   7%|▋         | 18/250 [00:15<03:18,  1.17it/s, loss=0.703]\u001b[A\n",
      "Train step of epoch 0:   7%|▋         | 18/250 [00:15<03:18,  1.17it/s, loss=0.612]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 19/250 [00:16<03:17,  1.17it/s, loss=0.612]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 19/250 [00:16<03:17,  1.17it/s, loss=0.662]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 20/250 [00:17<03:16,  1.17it/s, loss=0.662]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 20/250 [00:17<03:16,  1.17it/s, loss=0.625]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 21/250 [00:17<03:15,  1.17it/s, loss=0.625]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 21/250 [00:17<03:15,  1.17it/s, loss=0.653]\u001b[A\n",
      "Train step of epoch 0:   9%|▉         | 22/250 [00:18<03:15,  1.17it/s, loss=0.653]\u001b[A\n",
      "Train step of epoch 0:   9%|▉         | 22/250 [00:18<03:15,  1.17it/s, loss=0.679]\u001b[A\n",
      "Train step of epoch 0:   9%|▉         | 23/250 [00:19<03:14,  1.17it/s, loss=0.679]\u001b[A\n",
      "Train step of epoch 0:   9%|▉         | 23/250 [00:19<03:14,  1.17it/s, loss=0.798]\u001b[A\n",
      "Train step of epoch 0:  10%|▉         | 24/250 [00:20<03:13,  1.17it/s, loss=0.798]\u001b[A\n",
      "Train step of epoch 0:  10%|▉         | 24/250 [00:20<03:13,  1.17it/s, loss=0.622]\u001b[A\n",
      "Train step of epoch 0:  10%|█         | 25/250 [00:21<03:12,  1.17it/s, loss=0.622]\u001b[A\n",
      "Train step of epoch 0:  10%|█         | 25/250 [00:21<03:12,  1.17it/s, loss=0.623]\u001b[A\n",
      "Train step of epoch 0:  10%|█         | 26/250 [00:22<03:11,  1.17it/s, loss=0.623]\u001b[A\n",
      "Train step of epoch 0:  10%|█         | 26/250 [00:22<03:11,  1.17it/s, loss=0.721]\u001b[A\n",
      "Train step of epoch 0:  11%|█         | 27/250 [00:23<03:10,  1.17it/s, loss=0.721]\u001b[A\n",
      "Train step of epoch 0:  11%|█         | 27/250 [00:23<03:10,  1.17it/s, loss=0.615]\u001b[A\n",
      "Train step of epoch 0:  11%|█         | 28/250 [00:23<03:10,  1.17it/s, loss=0.615]\u001b[A\n",
      "Train step of epoch 0:  11%|█         | 28/250 [00:23<03:10,  1.17it/s, loss=0.576]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 29/250 [00:24<03:09,  1.17it/s, loss=0.576]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 29/250 [00:24<03:09,  1.17it/s, loss=0.482]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 30/250 [00:25<03:08,  1.17it/s, loss=0.482]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 30/250 [00:25<03:08,  1.17it/s, loss=0.578]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 31/250 [00:26<03:07,  1.17it/s, loss=0.578]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 31/250 [00:26<03:07,  1.17it/s, loss=0.467]\u001b[A\n",
      "Train step of epoch 0:  13%|█▎        | 32/250 [00:27<03:06,  1.17it/s, loss=0.467]\u001b[A\n",
      "Train step of epoch 0:  13%|█▎        | 32/250 [00:27<03:06,  1.17it/s, loss=0.865]\u001b[A\n",
      "Train step of epoch 0:  13%|█▎        | 33/250 [00:28<03:06,  1.17it/s, loss=0.865]\u001b[A\n",
      "Train step of epoch 0:  13%|█▎        | 33/250 [00:28<03:06,  1.17it/s, loss=0.612]\u001b[A\n",
      "Train step of epoch 0:  14%|█▎        | 34/250 [00:29<03:05,  1.16it/s, loss=0.612]\u001b[A\n",
      "Train step of epoch 0:  14%|█▎        | 34/250 [00:29<03:05,  1.16it/s, loss=0.452]\u001b[A\n",
      "Train step of epoch 0:  14%|█▍        | 35/250 [00:29<03:04,  1.16it/s, loss=0.452]\u001b[A\n",
      "Train step of epoch 0:  14%|█▍        | 35/250 [00:29<03:04,  1.16it/s, loss=0.824]\u001b[A\n",
      "Train step of epoch 0:  14%|█▍        | 36/250 [00:30<03:03,  1.16it/s, loss=0.824]\u001b[A\n",
      "Train step of epoch 0:  14%|█▍        | 36/250 [00:30<03:03,  1.16it/s, loss=1.11] \u001b[A\n",
      "Train step of epoch 0:  15%|█▍        | 37/250 [00:31<03:02,  1.16it/s, loss=1.11]\u001b[A\n",
      "Train step of epoch 0:  15%|█▍        | 37/250 [00:31<03:02,  1.16it/s, loss=0.848]\u001b[A\n",
      "Train step of epoch 0:  15%|█▌        | 38/250 [00:32<03:02,  1.16it/s, loss=0.848]\u001b[A\n",
      "Train step of epoch 0:  15%|█▌        | 38/250 [00:32<03:02,  1.16it/s, loss=0.458]\u001b[A\n",
      "Train step of epoch 0:  16%|█▌        | 39/250 [00:33<03:01,  1.16it/s, loss=0.458]\u001b[A\n",
      "Train step of epoch 0:  16%|█▌        | 39/250 [00:33<03:01,  1.16it/s, loss=0.725]\u001b[A\n",
      "Train step of epoch 0:  16%|█▌        | 40/250 [00:34<03:00,  1.16it/s, loss=0.725]\u001b[A\n",
      "Train step of epoch 0:  16%|█▌        | 40/250 [00:34<03:00,  1.16it/s, loss=0.395]\u001b[A\n",
      "Train step of epoch 0:  16%|█▋        | 41/250 [00:35<02:59,  1.16it/s, loss=0.395]\u001b[A\n",
      "Train step of epoch 0:  16%|█▋        | 41/250 [00:35<02:59,  1.16it/s, loss=0.423]\u001b[A\n",
      "Train step of epoch 0:  17%|█▋        | 42/250 [00:35<02:58,  1.16it/s, loss=0.423]\u001b[A\n",
      "Train step of epoch 0:  17%|█▋        | 42/250 [00:36<02:58,  1.16it/s, loss=0.646]\u001b[A\n",
      "Train step of epoch 0:  17%|█▋        | 43/250 [00:36<02:58,  1.16it/s, loss=0.646]\u001b[A\n",
      "Train step of epoch 0:  17%|█▋        | 43/250 [00:36<02:58,  1.16it/s, loss=0.781]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 44/250 [00:37<02:57,  1.16it/s, loss=0.781]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 44/250 [00:37<02:57,  1.16it/s, loss=0.987]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 45/250 [00:38<02:56,  1.16it/s, loss=0.987]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 45/250 [00:38<02:56,  1.16it/s, loss=0.61] \u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 46/250 [00:39<02:56,  1.16it/s, loss=0.61]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 46/250 [00:39<02:56,  1.16it/s, loss=0.437]\u001b[A\n",
      "Train step of epoch 0:  19%|█▉        | 47/250 [00:40<02:55,  1.16it/s, loss=0.437]\u001b[A\n",
      "Train step of epoch 0:  19%|█▉        | 47/250 [00:40<02:55,  1.16it/s, loss=0.672]\u001b[A\n",
      "Train step of epoch 0:  19%|█▉        | 48/250 [00:41<02:54,  1.16it/s, loss=0.672]\u001b[A\n",
      "Train step of epoch 0:  19%|█▉        | 48/250 [00:41<02:54,  1.16it/s, loss=0.669]\u001b[A\n",
      "Train step of epoch 0:  20%|█▉        | 49/250 [00:42<02:53,  1.16it/s, loss=0.669]\u001b[A\n",
      "Train step of epoch 0:  20%|█▉        | 49/250 [00:42<02:53,  1.16it/s, loss=0.612]\u001b[A\n",
      "Train step of epoch 0:  20%|██        | 50/250 [00:42<02:52,  1.16it/s, loss=0.612]\u001b[A\n",
      "Train step of epoch 0:  20%|██        | 50/250 [00:42<02:52,  1.16it/s, loss=0.847]\u001b[A\n",
      "Train step of epoch 0:  20%|██        | 51/250 [00:43<02:52,  1.15it/s, loss=0.847]\u001b[A\n",
      "Train step of epoch 0:  20%|██        | 51/250 [00:43<02:52,  1.15it/s, loss=0.457]\u001b[A\n",
      "Train step of epoch 0:  21%|██        | 52/250 [00:44<02:51,  1.15it/s, loss=0.457]\u001b[A\n",
      "Train step of epoch 0:  21%|██        | 52/250 [00:44<02:51,  1.15it/s, loss=0.571]\u001b[A\n",
      "Train step of epoch 0:  21%|██        | 53/250 [00:45<02:51,  1.15it/s, loss=0.571]\u001b[A\n",
      "Train step of epoch 0:  21%|██        | 53/250 [00:45<02:51,  1.15it/s, loss=0.411]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 54/250 [00:46<02:50,  1.15it/s, loss=0.411]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 54/250 [00:46<02:50,  1.15it/s, loss=0.333]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 55/250 [00:47<02:49,  1.15it/s, loss=0.333]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 55/250 [00:47<02:49,  1.15it/s, loss=0.568]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 56/250 [00:48<02:48,  1.15it/s, loss=0.568]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 56/250 [00:48<02:48,  1.15it/s, loss=0.563]\u001b[A\n",
      "Train step of epoch 0:  23%|██▎       | 57/250 [00:48<02:48,  1.15it/s, loss=0.563]\u001b[A\n",
      "Train step of epoch 0:  23%|██▎       | 57/250 [00:49<02:48,  1.15it/s, loss=0.539]\u001b[A\n",
      "Train step of epoch 0:  23%|██▎       | 58/250 [00:49<02:47,  1.15it/s, loss=0.539]\u001b[A\n",
      "Train step of epoch 0:  23%|██▎       | 58/250 [00:49<02:47,  1.15it/s, loss=0.584]\u001b[A\n",
      "Train step of epoch 0:  24%|██▎       | 59/250 [00:50<02:46,  1.14it/s, loss=0.584]\u001b[A\n",
      "Train step of epoch 0:  24%|██▎       | 59/250 [00:50<02:46,  1.14it/s, loss=0.652]\u001b[A\n",
      "Train step of epoch 0:  24%|██▍       | 60/250 [00:51<02:45,  1.15it/s, loss=0.652]\u001b[A\n",
      "Train step of epoch 0:  24%|██▍       | 60/250 [00:51<02:45,  1.15it/s, loss=0.326]\u001b[A\n",
      "Train step of epoch 0:  24%|██▍       | 61/250 [00:52<02:45,  1.14it/s, loss=0.326]\u001b[A\n",
      "Train step of epoch 0:  24%|██▍       | 61/250 [00:52<02:45,  1.14it/s, loss=1.22] \u001b[A\n",
      "Train step of epoch 0:  25%|██▍       | 62/250 [00:53<02:44,  1.14it/s, loss=1.22]\u001b[A\n",
      "Train step of epoch 0:  25%|██▍       | 62/250 [00:53<02:44,  1.14it/s, loss=0.175]\u001b[A\n",
      "Train step of epoch 0:  25%|██▌       | 63/250 [00:54<02:43,  1.14it/s, loss=0.175]\u001b[A\n",
      "Train step of epoch 0:  25%|██▌       | 63/250 [00:54<02:43,  1.14it/s, loss=0.858]\u001b[A\n",
      "Train step of epoch 0:  26%|██▌       | 64/250 [00:55<02:43,  1.14it/s, loss=0.858]\u001b[A\n",
      "Train step of epoch 0:  26%|██▌       | 64/250 [00:55<02:43,  1.14it/s, loss=0.346]\u001b[A\n",
      "Train step of epoch 0:  26%|██▌       | 65/250 [00:56<02:42,  1.14it/s, loss=0.346]\u001b[A\n",
      "Train step of epoch 0:  26%|██▌       | 65/250 [00:56<02:42,  1.14it/s, loss=0.767]\u001b[A\n",
      "Train step of epoch 0:  26%|██▋       | 66/250 [00:56<02:41,  1.14it/s, loss=0.767]\u001b[A\n",
      "Train step of epoch 0:  26%|██▋       | 66/250 [00:56<02:41,  1.14it/s, loss=0.188]\u001b[A\n",
      "Train step of epoch 0:  27%|██▋       | 67/250 [00:57<02:40,  1.14it/s, loss=0.188]\u001b[A\n",
      "Train step of epoch 0:  27%|██▋       | 67/250 [00:57<02:40,  1.14it/s, loss=0.861]\u001b[A\n",
      "Train step of epoch 0:  27%|██▋       | 68/250 [00:58<02:39,  1.14it/s, loss=0.861]\u001b[A\n",
      "Train step of epoch 0:  27%|██▋       | 68/250 [00:58<02:39,  1.14it/s, loss=0.38] \u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 69/250 [00:59<02:39,  1.14it/s, loss=0.38]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 69/250 [00:59<02:39,  1.14it/s, loss=0.791]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 70/250 [01:00<02:38,  1.14it/s, loss=0.791]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 70/250 [01:00<02:38,  1.14it/s, loss=0.731]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 71/250 [01:01<02:37,  1.14it/s, loss=0.731]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 71/250 [01:01<02:37,  1.14it/s, loss=0.528]\u001b[A\n",
      "Train step of epoch 0:  29%|██▉       | 72/250 [01:02<02:36,  1.14it/s, loss=0.528]\u001b[A\n",
      "Train step of epoch 0:  29%|██▉       | 72/250 [01:02<02:36,  1.14it/s, loss=0.775]\u001b[A\n",
      "Train step of epoch 0:  29%|██▉       | 73/250 [01:03<02:36,  1.13it/s, loss=0.775]\u001b[A\n",
      "Train step of epoch 0:  29%|██▉       | 73/250 [01:03<02:36,  1.13it/s, loss=0.714]\u001b[A\n",
      "Train step of epoch 0:  30%|██▉       | 74/250 [01:03<02:35,  1.13it/s, loss=0.714]\u001b[A\n",
      "Train step of epoch 0:  30%|██▉       | 74/250 [01:03<02:35,  1.13it/s, loss=0.591]\u001b[A\n",
      "Train step of epoch 0:  30%|███       | 75/250 [01:04<02:34,  1.14it/s, loss=0.591]\u001b[A\n",
      "Train step of epoch 0:  30%|███       | 75/250 [01:04<02:34,  1.14it/s, loss=0.638]\u001b[A\n",
      "Train step of epoch 0:  30%|███       | 76/250 [01:05<02:33,  1.13it/s, loss=0.638]\u001b[A\n",
      "Train step of epoch 0:  30%|███       | 76/250 [01:05<02:33,  1.13it/s, loss=0.723]\u001b[A\n",
      "Train step of epoch 0:  31%|███       | 77/250 [01:06<02:32,  1.13it/s, loss=0.723]\u001b[A\n",
      "Train step of epoch 0:  31%|███       | 77/250 [01:06<02:32,  1.13it/s, loss=0.444]\u001b[A\n",
      "Train step of epoch 0:  31%|███       | 78/250 [01:07<02:31,  1.13it/s, loss=0.444]\u001b[A\n",
      "Train step of epoch 0:  31%|███       | 78/250 [01:07<02:31,  1.13it/s, loss=0.628]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 79/250 [01:08<02:30,  1.14it/s, loss=0.628]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 79/250 [01:08<02:30,  1.14it/s, loss=0.549]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 80/250 [01:09<02:29,  1.13it/s, loss=0.549]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 80/250 [01:09<02:29,  1.13it/s, loss=0.689]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 81/250 [01:10<02:28,  1.14it/s, loss=0.689]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 81/250 [01:10<02:28,  1.14it/s, loss=0.408]\u001b[A\n",
      "Train step of epoch 0:  33%|███▎      | 82/250 [01:10<02:27,  1.14it/s, loss=0.408]\u001b[A\n",
      "Train step of epoch 0:  33%|███▎      | 82/250 [01:11<02:27,  1.14it/s, loss=0.833]\u001b[A\n",
      "Train step of epoch 0:  33%|███▎      | 83/250 [01:11<02:26,  1.14it/s, loss=0.833]\u001b[A\n",
      "Train step of epoch 0:  33%|███▎      | 83/250 [01:11<02:26,  1.14it/s, loss=0.485]\u001b[A\n",
      "Train step of epoch 0:  34%|███▎      | 84/250 [01:12<02:26,  1.14it/s, loss=0.485]\u001b[A\n",
      "Train step of epoch 0:  34%|███▎      | 84/250 [01:12<02:26,  1.14it/s, loss=0.637]\u001b[A\n",
      "Train step of epoch 0:  34%|███▍      | 85/250 [01:13<02:25,  1.14it/s, loss=0.637]\u001b[A\n",
      "Train step of epoch 0:  34%|███▍      | 85/250 [01:13<02:25,  1.14it/s, loss=0.727]\u001b[A\n",
      "Train step of epoch 0:  34%|███▍      | 86/250 [01:14<02:24,  1.14it/s, loss=0.727]\u001b[A\n",
      "Train step of epoch 0:  34%|███▍      | 86/250 [01:14<02:24,  1.14it/s, loss=0.622]\u001b[A\n",
      "Train step of epoch 0:  35%|███▍      | 87/250 [01:15<02:23,  1.14it/s, loss=0.622]\u001b[A\n",
      "Train step of epoch 0:  35%|███▍      | 87/250 [01:15<02:23,  1.14it/s, loss=0.465]\u001b[A\n",
      "Train step of epoch 0:  35%|███▌      | 88/250 [01:16<02:22,  1.14it/s, loss=0.465]\u001b[A\n",
      "Train step of epoch 0:  35%|███▌      | 88/250 [01:16<02:22,  1.14it/s, loss=0.919]\u001b[A\n",
      "Train step of epoch 0:  36%|███▌      | 89/250 [01:17<02:21,  1.14it/s, loss=0.919]\u001b[A\n",
      "Train step of epoch 0:  36%|███▌      | 89/250 [01:17<02:21,  1.14it/s, loss=0.821]\u001b[A\n",
      "Train step of epoch 0:  36%|███▌      | 90/250 [01:18<02:20,  1.14it/s, loss=0.821]\u001b[A\n",
      "Train step of epoch 0:  36%|███▌      | 90/250 [01:18<02:20,  1.14it/s, loss=0.757]\u001b[A\n",
      "Train step of epoch 0:  36%|███▋      | 91/250 [01:18<02:19,  1.14it/s, loss=0.757]\u001b[A\n",
      "Train step of epoch 0:  36%|███▋      | 91/250 [01:18<02:19,  1.14it/s, loss=0.61] \u001b[A\n",
      "Train step of epoch 0:  37%|███▋      | 92/250 [01:19<02:18,  1.14it/s, loss=0.61]\u001b[A\n",
      "Train step of epoch 0:  37%|███▋      | 92/250 [01:19<02:18,  1.14it/s, loss=0.595]\u001b[A\n",
      "Train step of epoch 0:  37%|███▋      | 93/250 [01:20<02:17,  1.14it/s, loss=0.595]\u001b[A\n",
      "Train step of epoch 0:  37%|███▋      | 93/250 [01:20<02:17,  1.14it/s, loss=0.856]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 94/250 [01:21<02:16,  1.14it/s, loss=0.856]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 94/250 [01:21<02:16,  1.14it/s, loss=0.725]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 95/250 [01:22<02:15,  1.14it/s, loss=0.725]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 95/250 [01:22<02:15,  1.14it/s, loss=0.495]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 96/250 [01:23<02:14,  1.14it/s, loss=0.495]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 96/250 [01:23<02:14,  1.14it/s, loss=0.703]\u001b[A\n",
      "Train step of epoch 0:  39%|███▉      | 97/250 [01:24<02:13,  1.14it/s, loss=0.703]\u001b[A\n",
      "Train step of epoch 0:  39%|███▉      | 97/250 [01:24<02:13,  1.14it/s, loss=0.811]\u001b[A\n",
      "Train step of epoch 0:  39%|███▉      | 98/250 [01:25<02:13,  1.14it/s, loss=0.811]\u001b[A\n",
      "Train step of epoch 0:  39%|███▉      | 98/250 [01:25<02:13,  1.14it/s, loss=0.756]\u001b[A\n",
      "Train step of epoch 0:  40%|███▉      | 99/250 [01:25<02:12,  1.14it/s, loss=0.756]\u001b[A\n",
      "Train step of epoch 0:  40%|███▉      | 99/250 [01:25<02:12,  1.14it/s, loss=0.603]\u001b[A\n",
      "Train step of epoch 0:  40%|████      | 100/250 [01:26<02:11,  1.14it/s, loss=0.603]\u001b[A\n",
      "Train step of epoch 0:  40%|████      | 100/250 [01:26<02:11,  1.14it/s, loss=0.657]\u001b[A\n",
      "Train step of epoch 0:  40%|████      | 101/250 [01:27<02:10,  1.15it/s, loss=0.657]\u001b[A\n",
      "Train step of epoch 0:  40%|████      | 101/250 [01:27<02:10,  1.15it/s, loss=0.7]  \u001b[A\n",
      "Train step of epoch 0:  41%|████      | 102/250 [01:28<02:09,  1.15it/s, loss=0.7]\u001b[A\n",
      "Train step of epoch 0:  41%|████      | 102/250 [01:28<02:09,  1.15it/s, loss=0.684]\u001b[A\n",
      "Train step of epoch 0:  41%|████      | 103/250 [01:29<02:08,  1.15it/s, loss=0.684]\u001b[A\n",
      "Train step of epoch 0:  41%|████      | 103/250 [01:29<02:08,  1.15it/s, loss=0.734]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 104/250 [01:30<02:07,  1.15it/s, loss=0.734]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 104/250 [01:30<02:07,  1.15it/s, loss=0.765]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 105/250 [01:31<02:06,  1.15it/s, loss=0.765]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 105/250 [01:31<02:06,  1.15it/s, loss=0.65] \u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 106/250 [01:31<02:05,  1.15it/s, loss=0.65]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 106/250 [01:32<02:05,  1.15it/s, loss=0.682]\u001b[A\n",
      "Train step of epoch 0:  43%|████▎     | 107/250 [01:32<02:04,  1.15it/s, loss=0.682]\u001b[A\n",
      "Train step of epoch 0:  43%|████▎     | 107/250 [01:32<02:04,  1.15it/s, loss=0.725]\u001b[A\n",
      "Train step of epoch 0:  43%|████▎     | 108/250 [01:33<02:03,  1.15it/s, loss=0.725]\u001b[A\n",
      "Train step of epoch 0:  43%|████▎     | 108/250 [01:33<02:03,  1.15it/s, loss=0.606]\u001b[A\n",
      "Train step of epoch 0:  44%|████▎     | 109/250 [01:34<02:02,  1.15it/s, loss=0.606]\u001b[A\n",
      "Train step of epoch 0:  44%|████▎     | 109/250 [01:34<02:02,  1.15it/s, loss=0.66] \u001b[A\n",
      "Train step of epoch 0:  44%|████▍     | 110/250 [01:35<02:01,  1.15it/s, loss=0.66]\u001b[A\n",
      "Train step of epoch 0:  44%|████▍     | 110/250 [01:35<02:01,  1.15it/s, loss=0.757]\u001b[A\n",
      "Train step of epoch 0:  44%|████▍     | 111/250 [01:36<02:00,  1.15it/s, loss=0.757]\u001b[A\n",
      "Train step of epoch 0:  44%|████▍     | 111/250 [01:36<02:00,  1.15it/s, loss=0.605]\u001b[A\n",
      "Train step of epoch 0:  45%|████▍     | 112/250 [01:37<01:59,  1.15it/s, loss=0.605]\u001b[A\n",
      "Train step of epoch 0:  45%|████▍     | 112/250 [01:37<01:59,  1.15it/s, loss=0.656]\u001b[A\n",
      "Train step of epoch 0:  45%|████▌     | 113/250 [01:38<01:58,  1.15it/s, loss=0.656]\u001b[A\n",
      "Train step of epoch 0:  45%|████▌     | 113/250 [01:38<01:58,  1.15it/s, loss=0.656]\u001b[A\n",
      "Train step of epoch 0:  46%|████▌     | 114/250 [01:38<01:58,  1.15it/s, loss=0.656]\u001b[A\n",
      "Train step of epoch 0:  46%|████▌     | 114/250 [01:38<01:58,  1.15it/s, loss=0.829]\u001b[A\n",
      "Train step of epoch 0:  46%|████▌     | 115/250 [01:39<01:57,  1.15it/s, loss=0.829]\u001b[A\n",
      "Train step of epoch 0:  46%|████▌     | 115/250 [01:39<01:57,  1.15it/s, loss=0.668]\u001b[A\n",
      "Train step of epoch 0:  46%|████▋     | 116/250 [01:40<01:56,  1.15it/s, loss=0.668]\u001b[A\n",
      "Train step of epoch 0:  46%|████▋     | 116/250 [01:40<01:56,  1.15it/s, loss=0.588]\u001b[A\n",
      "Train step of epoch 0:  47%|████▋     | 117/250 [01:41<01:55,  1.15it/s, loss=0.588]\u001b[A\n",
      "Train step of epoch 0:  47%|████▋     | 117/250 [01:41<01:55,  1.15it/s, loss=0.558]\u001b[A\n",
      "Train step of epoch 0:  47%|████▋     | 118/250 [01:42<01:54,  1.15it/s, loss=0.558]\u001b[A\n",
      "Train step of epoch 0:  47%|████▋     | 118/250 [01:42<01:54,  1.15it/s, loss=0.502]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 119/250 [01:43<01:53,  1.16it/s, loss=0.502]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 119/250 [01:43<01:53,  1.16it/s, loss=0.522]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 120/250 [01:44<01:52,  1.15it/s, loss=0.522]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 120/250 [01:44<01:52,  1.15it/s, loss=0.997]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 121/250 [01:44<01:51,  1.16it/s, loss=0.997]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 121/250 [01:45<01:51,  1.16it/s, loss=0.786]\u001b[A\n",
      "Train step of epoch 0:  49%|████▉     | 122/250 [01:45<01:50,  1.16it/s, loss=0.786]\u001b[A\n",
      "Train step of epoch 0:  49%|████▉     | 122/250 [01:45<01:50,  1.16it/s, loss=0.662]\u001b[A\n",
      "Train step of epoch 0:  49%|████▉     | 123/250 [01:46<01:49,  1.15it/s, loss=0.662]\u001b[A\n",
      "Train step of epoch 0:  49%|████▉     | 123/250 [01:46<01:49,  1.15it/s, loss=0.549]\u001b[A\n",
      "Train step of epoch 0:  50%|████▉     | 124/250 [01:47<01:49,  1.15it/s, loss=0.549]\u001b[A\n",
      "Train step of epoch 0:  50%|████▉     | 124/250 [01:47<01:49,  1.15it/s, loss=0.747]\u001b[A\n",
      "Train step of epoch 0:  50%|█████     | 125/250 [01:48<01:48,  1.15it/s, loss=0.747]\u001b[A\n",
      "Train step of epoch 0:  50%|█████     | 125/250 [01:48<01:48,  1.15it/s, loss=0.663]\u001b[A\n",
      "Train step of epoch 0:  50%|█████     | 126/250 [01:49<01:47,  1.16it/s, loss=0.663]\u001b[A\n",
      "Train step of epoch 0:  50%|█████     | 126/250 [01:49<01:47,  1.16it/s, loss=0.698]\u001b[A\n",
      "Train step of epoch 0:  51%|█████     | 127/250 [01:50<01:46,  1.15it/s, loss=0.698]\u001b[A\n",
      "Train step of epoch 0:  51%|█████     | 127/250 [01:50<01:46,  1.15it/s, loss=0.568]\u001b[A\n",
      "Train step of epoch 0:  51%|█████     | 128/250 [01:51<01:45,  1.15it/s, loss=0.568]\u001b[A\n",
      "Train step of epoch 0:  51%|█████     | 128/250 [01:51<01:45,  1.15it/s, loss=0.627]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 129/250 [01:51<01:44,  1.15it/s, loss=0.627]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 129/250 [01:51<01:44,  1.15it/s, loss=0.641]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 130/250 [01:52<01:43,  1.15it/s, loss=0.641]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 130/250 [01:52<01:43,  1.15it/s, loss=0.571]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 131/250 [01:53<01:43,  1.15it/s, loss=0.571]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 131/250 [01:53<01:43,  1.15it/s, loss=0.909]\u001b[A\n",
      "Train step of epoch 0:  53%|█████▎    | 132/250 [01:54<01:42,  1.16it/s, loss=0.909]\u001b[A\n",
      "Train step of epoch 0:  53%|█████▎    | 132/250 [01:54<01:42,  1.16it/s, loss=0.597]\u001b[A\n",
      "Train step of epoch 0:  53%|█████▎    | 133/250 [01:55<01:41,  1.16it/s, loss=0.597]\u001b[A\n",
      "Train step of epoch 0:  53%|█████▎    | 133/250 [01:55<01:41,  1.16it/s, loss=0.784]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▎    | 134/250 [01:56<01:40,  1.16it/s, loss=0.784]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▎    | 134/250 [01:56<01:40,  1.16it/s, loss=0.73] \u001b[A\n",
      "Train step of epoch 0:  54%|█████▍    | 135/250 [01:57<01:39,  1.16it/s, loss=0.73]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▍    | 135/250 [01:57<01:39,  1.16it/s, loss=0.669]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▍    | 136/250 [01:57<01:38,  1.16it/s, loss=0.669]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▍    | 136/250 [01:57<01:38,  1.16it/s, loss=0.679]\u001b[A\n",
      "Train step of epoch 0:  55%|█████▍    | 137/250 [01:58<01:37,  1.16it/s, loss=0.679]\u001b[A\n",
      "Train step of epoch 0:  55%|█████▍    | 137/250 [01:58<01:37,  1.16it/s, loss=0.525]\u001b[A\n",
      "Train step of epoch 0:  55%|█████▌    | 138/250 [01:59<01:36,  1.16it/s, loss=0.525]\u001b[A\n",
      "Train step of epoch 0:  55%|█████▌    | 138/250 [01:59<01:36,  1.16it/s, loss=0.653]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▌    | 139/250 [02:00<01:36,  1.16it/s, loss=0.653]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▌    | 139/250 [02:00<01:36,  1.16it/s, loss=0.525]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▌    | 140/250 [02:01<01:35,  1.15it/s, loss=0.525]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▌    | 140/250 [02:01<01:35,  1.15it/s, loss=0.494]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▋    | 141/250 [02:02<01:34,  1.15it/s, loss=0.494]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▋    | 141/250 [02:02<01:34,  1.15it/s, loss=0.622]\u001b[A\n",
      "Train step of epoch 0:  57%|█████▋    | 142/250 [02:03<01:33,  1.15it/s, loss=0.622]\u001b[A\n",
      "Train step of epoch 0:  57%|█████▋    | 142/250 [02:03<01:33,  1.15it/s, loss=0.867]\u001b[A\n",
      "Train step of epoch 0:  57%|█████▋    | 143/250 [02:04<01:32,  1.15it/s, loss=0.867]\u001b[A\n",
      "Train step of epoch 0:  57%|█████▋    | 143/250 [02:04<01:32,  1.15it/s, loss=0.551]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 144/250 [02:04<01:31,  1.15it/s, loss=0.551]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 144/250 [02:04<01:31,  1.15it/s, loss=0.513]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 145/250 [02:05<01:30,  1.15it/s, loss=0.513]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 145/250 [02:05<01:30,  1.15it/s, loss=0.655]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 146/250 [02:06<01:30,  1.15it/s, loss=0.655]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 146/250 [02:06<01:30,  1.15it/s, loss=0.594]\u001b[A\n",
      "Train step of epoch 0:  59%|█████▉    | 147/250 [02:07<01:29,  1.15it/s, loss=0.594]\u001b[A\n",
      "Train step of epoch 0:  59%|█████▉    | 147/250 [02:07<01:29,  1.15it/s, loss=0.383]\u001b[A\n",
      "Train step of epoch 0:  59%|█████▉    | 148/250 [02:08<01:28,  1.15it/s, loss=0.383]\u001b[A\n",
      "Train step of epoch 0:  59%|█████▉    | 148/250 [02:08<01:28,  1.15it/s, loss=0.614]\u001b[A\n",
      "Train step of epoch 0:  60%|█████▉    | 149/250 [02:09<01:27,  1.15it/s, loss=0.614]\u001b[A\n",
      "Train step of epoch 0:  60%|█████▉    | 149/250 [02:09<01:27,  1.15it/s, loss=0.667]\u001b[A\n",
      "Train step of epoch 0:  60%|██████    | 150/250 [02:10<01:26,  1.15it/s, loss=0.667]\u001b[A\n",
      "Train step of epoch 0:  60%|██████    | 150/250 [02:10<01:26,  1.15it/s, loss=0.607]\u001b[A\n",
      "Train step of epoch 0:  60%|██████    | 151/250 [02:10<01:25,  1.15it/s, loss=0.607]\u001b[A\n",
      "Train step of epoch 0:  60%|██████    | 151/250 [02:11<01:25,  1.15it/s, loss=0.222]\u001b[A\n",
      "Train step of epoch 0:  61%|██████    | 152/250 [02:11<01:25,  1.15it/s, loss=0.222]\u001b[A\n",
      "Train step of epoch 0:  61%|██████    | 152/250 [02:11<01:25,  1.15it/s, loss=0.517]\u001b[A\n",
      "Train step of epoch 0:  61%|██████    | 153/250 [02:12<01:24,  1.15it/s, loss=0.517]\u001b[A\n",
      "Train step of epoch 0:  61%|██████    | 153/250 [02:12<01:24,  1.15it/s, loss=0.596]\u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 154/250 [02:13<01:23,  1.15it/s, loss=0.596]\u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 154/250 [02:13<01:23,  1.15it/s, loss=1.11] \u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 155/250 [02:14<01:22,  1.15it/s, loss=1.11]\u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 155/250 [02:14<01:22,  1.15it/s, loss=1.16]\u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 156/250 [02:15<01:21,  1.15it/s, loss=1.16]\u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 156/250 [02:15<01:21,  1.15it/s, loss=0.415]\u001b[A\n",
      "Train step of epoch 0:  63%|██████▎   | 157/250 [02:16<01:20,  1.15it/s, loss=0.415]\u001b[A\n",
      "Train step of epoch 0:  63%|██████▎   | 157/250 [02:16<01:20,  1.15it/s, loss=0.483]\u001b[A\n",
      "Train step of epoch 0:  63%|██████▎   | 158/250 [02:17<01:20,  1.15it/s, loss=0.483]\u001b[A\n",
      "Train step of epoch 0:  63%|██████▎   | 158/250 [02:17<01:20,  1.15it/s, loss=0.694]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▎   | 159/250 [02:17<01:19,  1.15it/s, loss=0.694]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▎   | 159/250 [02:17<01:19,  1.15it/s, loss=0.762]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▍   | 160/250 [02:18<01:18,  1.15it/s, loss=0.762]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▍   | 160/250 [02:18<01:18,  1.15it/s, loss=0.426]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▍   | 161/250 [02:19<01:17,  1.15it/s, loss=0.426]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▍   | 161/250 [02:19<01:17,  1.15it/s, loss=0.669]\u001b[A\n",
      "Train step of epoch 0:  65%|██████▍   | 162/250 [02:20<01:16,  1.15it/s, loss=0.669]\u001b[A\n",
      "Train step of epoch 0:  65%|██████▍   | 162/250 [02:20<01:16,  1.15it/s, loss=0.5]  \u001b[A\n",
      "Train step of epoch 0:  65%|██████▌   | 163/250 [02:21<01:15,  1.15it/s, loss=0.5]\u001b[A\n",
      "Train step of epoch 0:  65%|██████▌   | 163/250 [02:21<01:15,  1.15it/s, loss=0.437]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▌   | 164/250 [02:22<01:14,  1.15it/s, loss=0.437]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▌   | 164/250 [02:22<01:14,  1.15it/s, loss=0.621]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▌   | 165/250 [02:23<01:13,  1.15it/s, loss=0.621]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▌   | 165/250 [02:23<01:13,  1.15it/s, loss=0.971]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▋   | 166/250 [02:24<01:13,  1.15it/s, loss=0.971]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▋   | 166/250 [02:24<01:13,  1.15it/s, loss=0.353]\u001b[A\n",
      "Train step of epoch 0:  67%|██████▋   | 167/250 [02:24<01:12,  1.15it/s, loss=0.353]\u001b[A\n",
      "Train step of epoch 0:  67%|██████▋   | 167/250 [02:24<01:12,  1.15it/s, loss=0.431]\u001b[A\n",
      "Train step of epoch 0:  67%|██████▋   | 168/250 [02:25<01:11,  1.15it/s, loss=0.431]\u001b[A\n",
      "Train step of epoch 0:  67%|██████▋   | 168/250 [02:25<01:11,  1.15it/s, loss=0.488]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 169/250 [02:26<01:10,  1.15it/s, loss=0.488]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 169/250 [02:26<01:10,  1.15it/s, loss=0.437]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 170/250 [02:27<01:09,  1.15it/s, loss=0.437]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 170/250 [02:27<01:09,  1.15it/s, loss=0.896]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 171/250 [02:28<01:08,  1.15it/s, loss=0.896]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 171/250 [02:28<01:08,  1.15it/s, loss=0.642]\u001b[A\n",
      "Train step of epoch 0:  69%|██████▉   | 172/250 [02:29<01:07,  1.15it/s, loss=0.642]\u001b[A\n",
      "Train step of epoch 0:  69%|██████▉   | 172/250 [02:29<01:07,  1.15it/s, loss=0.818]\u001b[A\n",
      "Train step of epoch 0:  69%|██████▉   | 173/250 [02:30<01:07,  1.15it/s, loss=0.818]\u001b[A\n",
      "Train step of epoch 0:  69%|██████▉   | 173/250 [02:30<01:07,  1.15it/s, loss=0.434]\u001b[A\n",
      "Train step of epoch 0:  70%|██████▉   | 174/250 [02:31<01:06,  1.15it/s, loss=0.434]\u001b[A\n",
      "Train step of epoch 0:  70%|██████▉   | 174/250 [02:31<01:06,  1.15it/s, loss=0.412]\u001b[A\n",
      "Train step of epoch 0:  70%|███████   | 175/250 [02:31<01:05,  1.15it/s, loss=0.412]\u001b[A\n",
      "Train step of epoch 0:  70%|███████   | 175/250 [02:31<01:05,  1.15it/s, loss=0.62] \u001b[A\n",
      "Train step of epoch 0:  70%|███████   | 176/250 [02:32<01:04,  1.15it/s, loss=0.62]\u001b[A\n",
      "Train step of epoch 0:  70%|███████   | 176/250 [02:32<01:04,  1.15it/s, loss=0.453]\u001b[A\n",
      "Train step of epoch 0:  71%|███████   | 177/250 [02:33<01:03,  1.15it/s, loss=0.453]\u001b[A\n",
      "Train step of epoch 0:  71%|███████   | 177/250 [02:33<01:03,  1.15it/s, loss=0.365]\u001b[A\n",
      "Train step of epoch 0:  71%|███████   | 178/250 [02:34<01:02,  1.15it/s, loss=0.365]\u001b[A\n",
      "Train step of epoch 0:  71%|███████   | 178/250 [02:34<01:02,  1.15it/s, loss=0.371]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 179/250 [02:35<01:01,  1.15it/s, loss=0.371]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 179/250 [02:35<01:01,  1.15it/s, loss=0.545]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 180/250 [02:36<01:01,  1.15it/s, loss=0.545]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 180/250 [02:36<01:01,  1.15it/s, loss=0.707]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 181/250 [02:37<01:00,  1.15it/s, loss=0.707]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 181/250 [02:37<01:00,  1.15it/s, loss=0.268]\u001b[A\n",
      "Train step of epoch 0:  73%|███████▎  | 182/250 [02:37<00:59,  1.15it/s, loss=0.268]\u001b[A\n",
      "Train step of epoch 0:  73%|███████▎  | 182/250 [02:38<00:59,  1.15it/s, loss=0.987]\u001b[A\n",
      "Train step of epoch 0:  73%|███████▎  | 183/250 [02:38<00:58,  1.15it/s, loss=0.987]\u001b[A\n",
      "Train step of epoch 0:  73%|███████▎  | 183/250 [02:38<00:58,  1.15it/s, loss=1.14] \u001b[A\n",
      "Train step of epoch 0:  74%|███████▎  | 184/250 [02:39<00:57,  1.15it/s, loss=1.14]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▎  | 184/250 [02:39<00:57,  1.15it/s, loss=1.02]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▍  | 185/250 [02:40<00:56,  1.15it/s, loss=1.02]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▍  | 185/250 [02:40<00:56,  1.15it/s, loss=0.559]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▍  | 186/250 [02:41<00:55,  1.15it/s, loss=0.559]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▍  | 186/250 [02:41<00:55,  1.15it/s, loss=0.378]\u001b[A\n",
      "Train step of epoch 0:  75%|███████▍  | 187/250 [02:42<00:55,  1.15it/s, loss=0.378]\u001b[A\n",
      "Train step of epoch 0:  75%|███████▍  | 187/250 [02:42<00:55,  1.15it/s, loss=0.378]\u001b[A\n",
      "Train step of epoch 0:  75%|███████▌  | 188/250 [02:43<00:54,  1.14it/s, loss=0.378]\u001b[A\n",
      "Train step of epoch 0:  75%|███████▌  | 188/250 [02:43<00:54,  1.14it/s, loss=0.648]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▌  | 189/250 [02:44<00:53,  1.15it/s, loss=0.648]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▌  | 189/250 [02:44<00:53,  1.15it/s, loss=0.869]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▌  | 190/250 [02:44<00:52,  1.15it/s, loss=0.869]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▌  | 190/250 [02:44<00:52,  1.15it/s, loss=0.398]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▋  | 191/250 [02:45<00:51,  1.15it/s, loss=0.398]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▋  | 191/250 [02:45<00:51,  1.15it/s, loss=0.28] \u001b[A\n",
      "Train step of epoch 0:  77%|███████▋  | 192/250 [02:46<00:50,  1.15it/s, loss=0.28]\u001b[A\n",
      "Train step of epoch 0:  77%|███████▋  | 192/250 [02:46<00:50,  1.15it/s, loss=0.469]\u001b[A\n",
      "Train step of epoch 0:  77%|███████▋  | 193/250 [02:47<00:49,  1.15it/s, loss=0.469]\u001b[A\n",
      "Train step of epoch 0:  77%|███████▋  | 193/250 [02:47<00:49,  1.15it/s, loss=0.858]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 194/250 [02:48<00:48,  1.15it/s, loss=0.858]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 194/250 [02:48<00:48,  1.15it/s, loss=0.694]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 195/250 [02:49<00:47,  1.15it/s, loss=0.694]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 195/250 [02:49<00:47,  1.15it/s, loss=0.443]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 196/250 [02:50<00:47,  1.15it/s, loss=0.443]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 196/250 [02:50<00:47,  1.15it/s, loss=1.02] \u001b[A\n",
      "Train step of epoch 0:  79%|███████▉  | 197/250 [02:51<00:46,  1.15it/s, loss=1.02]\u001b[A\n",
      "Train step of epoch 0:  79%|███████▉  | 197/250 [02:51<00:46,  1.15it/s, loss=0.675]\u001b[A\n",
      "Train step of epoch 0:  79%|███████▉  | 198/250 [02:51<00:45,  1.15it/s, loss=0.675]\u001b[A\n",
      "Train step of epoch 0:  79%|███████▉  | 198/250 [02:51<00:45,  1.15it/s, loss=0.557]\u001b[A\n",
      "Train step of epoch 0:  80%|███████▉  | 199/250 [02:52<00:44,  1.15it/s, loss=0.557]\u001b[A\n",
      "Train step of epoch 0:  80%|███████▉  | 199/250 [02:52<00:44,  1.15it/s, loss=0.559]\u001b[A\n",
      "Train step of epoch 0:  80%|████████  | 200/250 [02:53<00:43,  1.15it/s, loss=0.559]\u001b[A\n",
      "Train step of epoch 0:  80%|████████  | 200/250 [02:53<00:43,  1.15it/s, loss=0.47] \u001b[A\n",
      "Train step of epoch 0:  80%|████████  | 201/250 [02:54<00:42,  1.15it/s, loss=0.47]\u001b[A\n",
      "Train step of epoch 0:  80%|████████  | 201/250 [02:54<00:42,  1.15it/s, loss=0.486]\u001b[A\n",
      "Train step of epoch 0:  81%|████████  | 202/250 [02:55<00:41,  1.15it/s, loss=0.486]\u001b[A\n",
      "Train step of epoch 0:  81%|████████  | 202/250 [02:55<00:41,  1.15it/s, loss=0.771]\u001b[A\n",
      "Train step of epoch 0:  81%|████████  | 203/250 [02:56<00:40,  1.15it/s, loss=0.771]\u001b[A\n",
      "Train step of epoch 0:  81%|████████  | 203/250 [02:56<00:40,  1.15it/s, loss=0.637]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 204/250 [02:57<00:39,  1.15it/s, loss=0.637]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 204/250 [02:57<00:39,  1.15it/s, loss=0.495]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 205/250 [02:58<00:39,  1.15it/s, loss=0.495]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 205/250 [02:58<00:39,  1.15it/s, loss=0.562]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 206/250 [02:58<00:38,  1.15it/s, loss=0.562]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 206/250 [02:58<00:38,  1.15it/s, loss=0.772]\u001b[A\n",
      "Train step of epoch 0:  83%|████████▎ | 207/250 [02:59<00:37,  1.15it/s, loss=0.772]\u001b[A\n",
      "Train step of epoch 0:  83%|████████▎ | 207/250 [02:59<00:37,  1.15it/s, loss=0.55] \u001b[A\n",
      "Train step of epoch 0:  83%|████████▎ | 208/250 [03:00<00:36,  1.15it/s, loss=0.55]\u001b[A\n",
      "Train step of epoch 0:  83%|████████▎ | 208/250 [03:00<00:36,  1.15it/s, loss=0.72]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▎ | 209/250 [03:01<00:35,  1.15it/s, loss=0.72]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▎ | 209/250 [03:01<00:35,  1.15it/s, loss=0.688]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▍ | 210/250 [03:02<00:34,  1.15it/s, loss=0.688]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▍ | 210/250 [03:02<00:34,  1.15it/s, loss=0.585]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▍ | 211/250 [03:03<00:33,  1.15it/s, loss=0.585]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▍ | 211/250 [03:03<00:33,  1.15it/s, loss=0.408]\u001b[A\n",
      "Train step of epoch 0:  85%|████████▍ | 212/250 [03:04<00:33,  1.15it/s, loss=0.408]\u001b[A\n",
      "Train step of epoch 0:  85%|████████▍ | 212/250 [03:04<00:33,  1.15it/s, loss=0.574]\u001b[A\n",
      "Train step of epoch 0:  85%|████████▌ | 213/250 [03:04<00:32,  1.15it/s, loss=0.574]\u001b[A\n",
      "Train step of epoch 0:  85%|████████▌ | 213/250 [03:05<00:32,  1.15it/s, loss=0.375]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▌ | 214/250 [03:05<00:31,  1.15it/s, loss=0.375]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▌ | 214/250 [03:05<00:31,  1.15it/s, loss=1.1]  \u001b[A\n",
      "Train step of epoch 0:  86%|████████▌ | 215/250 [03:06<00:30,  1.15it/s, loss=1.1]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▌ | 215/250 [03:06<00:30,  1.15it/s, loss=0.488]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▋ | 216/250 [03:07<00:29,  1.15it/s, loss=0.488]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▋ | 216/250 [03:07<00:29,  1.15it/s, loss=0.853]\u001b[A\n",
      "Train step of epoch 0:  87%|████████▋ | 217/250 [03:08<00:28,  1.15it/s, loss=0.853]\u001b[A\n",
      "Train step of epoch 0:  87%|████████▋ | 217/250 [03:08<00:28,  1.15it/s, loss=0.814]\u001b[A\n",
      "Train step of epoch 0:  87%|████████▋ | 218/250 [03:09<00:27,  1.15it/s, loss=0.814]\u001b[A\n",
      "Train step of epoch 0:  87%|████████▋ | 218/250 [03:09<00:27,  1.15it/s, loss=0.561]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 219/250 [03:10<00:26,  1.15it/s, loss=0.561]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 219/250 [03:10<00:26,  1.15it/s, loss=0.392]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 220/250 [03:11<00:26,  1.15it/s, loss=0.392]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 220/250 [03:11<00:26,  1.15it/s, loss=0.621]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 221/250 [03:11<00:25,  1.15it/s, loss=0.621]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 221/250 [03:11<00:25,  1.15it/s, loss=0.626]\u001b[A\n",
      "Train step of epoch 0:  89%|████████▉ | 222/250 [03:12<00:24,  1.15it/s, loss=0.626]\u001b[A\n",
      "Train step of epoch 0:  89%|████████▉ | 222/250 [03:12<00:24,  1.15it/s, loss=0.331]\u001b[A\n",
      "Train step of epoch 0:  89%|████████▉ | 223/250 [03:13<00:23,  1.15it/s, loss=0.331]\u001b[A\n",
      "Train step of epoch 0:  89%|████████▉ | 223/250 [03:13<00:23,  1.15it/s, loss=0.756]\u001b[A\n",
      "Train step of epoch 0:  90%|████████▉ | 224/250 [03:14<00:22,  1.15it/s, loss=0.756]\u001b[A\n",
      "Train step of epoch 0:  90%|████████▉ | 224/250 [03:14<00:22,  1.15it/s, loss=0.764]\u001b[A\n",
      "Train step of epoch 0:  90%|█████████ | 225/250 [03:15<00:21,  1.15it/s, loss=0.764]\u001b[A\n",
      "Train step of epoch 0:  90%|█████████ | 225/250 [03:15<00:21,  1.15it/s, loss=0.609]\u001b[A\n",
      "Train step of epoch 0:  90%|█████████ | 226/250 [03:16<00:20,  1.15it/s, loss=0.609]\u001b[A\n",
      "Train step of epoch 0:  90%|█████████ | 226/250 [03:16<00:20,  1.15it/s, loss=0.681]\u001b[A\n",
      "Train step of epoch 0:  91%|█████████ | 227/250 [03:17<00:19,  1.15it/s, loss=0.681]\u001b[A\n",
      "Train step of epoch 0:  91%|█████████ | 227/250 [03:17<00:19,  1.15it/s, loss=0.695]\u001b[A\n",
      "Train step of epoch 0:  91%|█████████ | 228/250 [03:18<00:19,  1.15it/s, loss=0.695]\u001b[A\n",
      "Train step of epoch 0:  91%|█████████ | 228/250 [03:18<00:19,  1.15it/s, loss=0.492]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 229/250 [03:18<00:18,  1.15it/s, loss=0.492]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 229/250 [03:18<00:18,  1.15it/s, loss=0.701]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 230/250 [03:19<00:17,  1.15it/s, loss=0.701]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 230/250 [03:19<00:17,  1.15it/s, loss=0.695]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 231/250 [03:20<00:16,  1.15it/s, loss=0.695]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 231/250 [03:20<00:16,  1.15it/s, loss=0.386]\u001b[A\n",
      "Train step of epoch 0:  93%|█████████▎| 232/250 [03:21<00:15,  1.15it/s, loss=0.386]\u001b[A\n",
      "Train step of epoch 0:  93%|█████████▎| 232/250 [03:21<00:15,  1.15it/s, loss=0.841]\u001b[A\n",
      "Train step of epoch 0:  93%|█████████▎| 233/250 [03:22<00:14,  1.15it/s, loss=0.841]\u001b[A\n",
      "Train step of epoch 0:  93%|█████████▎| 233/250 [03:22<00:14,  1.15it/s, loss=0.501]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▎| 234/250 [03:23<00:13,  1.15it/s, loss=0.501]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▎| 234/250 [03:23<00:13,  1.15it/s, loss=0.573]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▍| 235/250 [03:24<00:13,  1.15it/s, loss=0.573]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▍| 235/250 [03:24<00:13,  1.15it/s, loss=0.978]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▍| 236/250 [03:24<00:12,  1.15it/s, loss=0.978]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▍| 236/250 [03:25<00:12,  1.15it/s, loss=0.484]\u001b[A\n",
      "Train step of epoch 0:  95%|█████████▍| 237/250 [03:25<00:11,  1.15it/s, loss=0.484]\u001b[A\n",
      "Train step of epoch 0:  95%|█████████▍| 237/250 [03:25<00:11,  1.15it/s, loss=0.491]\u001b[A\n",
      "Train step of epoch 0:  95%|█████████▌| 238/250 [03:26<00:10,  1.15it/s, loss=0.491]\u001b[A\n",
      "Train step of epoch 0:  95%|█████████▌| 238/250 [03:26<00:10,  1.15it/s, loss=0.739]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▌| 239/250 [03:27<00:09,  1.15it/s, loss=0.739]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▌| 239/250 [03:27<00:09,  1.15it/s, loss=0.688]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▌| 240/250 [03:28<00:08,  1.15it/s, loss=0.688]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▌| 240/250 [03:28<00:08,  1.15it/s, loss=0.611]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▋| 241/250 [03:29<00:07,  1.15it/s, loss=0.611]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▋| 241/250 [03:29<00:07,  1.15it/s, loss=0.485]\u001b[A\n",
      "Train step of epoch 0:  97%|█████████▋| 242/250 [03:30<00:06,  1.15it/s, loss=0.485]\u001b[A\n",
      "Train step of epoch 0:  97%|█████████▋| 242/250 [03:30<00:06,  1.15it/s, loss=0.878]\u001b[A\n",
      "Train step of epoch 0:  97%|█████████▋| 243/250 [03:31<00:06,  1.15it/s, loss=0.878]\u001b[A\n",
      "Train step of epoch 0:  97%|█████████▋| 243/250 [03:31<00:06,  1.15it/s, loss=0.594]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 244/250 [03:31<00:05,  1.15it/s, loss=0.594]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 244/250 [03:31<00:05,  1.15it/s, loss=0.638]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 245/250 [03:32<00:04,  1.15it/s, loss=0.638]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 245/250 [03:32<00:04,  1.15it/s, loss=0.835]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 246/250 [03:33<00:03,  1.15it/s, loss=0.835]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 246/250 [03:33<00:03,  1.15it/s, loss=0.76] \u001b[A\n",
      "Train step of epoch 0:  99%|█████████▉| 247/250 [03:34<00:02,  1.15it/s, loss=0.76]\u001b[A\n",
      "Train step of epoch 0:  99%|█████████▉| 247/250 [03:34<00:02,  1.15it/s, loss=0.358]\u001b[A\n",
      "Train step of epoch 0:  99%|█████████▉| 248/250 [03:35<00:01,  1.15it/s, loss=0.358]\u001b[A\n",
      "Train step of epoch 0:  99%|█████████▉| 248/250 [03:35<00:01,  1.15it/s, loss=1.22] \u001b[A\n",
      "Train step of epoch 0: 100%|█████████▉| 249/250 [03:36<00:00,  1.15it/s, loss=1.22]\u001b[A\n",
      "Train step of epoch 0: 100%|█████████▉| 249/250 [03:36<00:00,  1.15it/s, loss=0.638]\u001b[A\n",
      "Train step of epoch 0: 100%|██████████| 250/250 [03:37<00:00,  1.15it/s, loss=0.638]\u001b[A\n",
      "Train epoch: 100%|██████████| 1/1 [03:51<00:00, 231.31s/it]0,  1.15it/s, loss=0.446]\u001b[A\n",
      "Train step of epoch 0: 100%|██████████| 250/250 [03:51<00:00,  1.08it/s, loss=0.619, dist_mean=0.269]\u001b[A\n",
      "Train epoch: 100%|██████████| 1/1 [03:51<00:00, 231.31s/it]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(use_lora=0)\n",
    "\n",
    "model.save_pretrained('/aiffel/KoChatGPT/output_2_RM')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d01a4b",
   "metadata": {},
   "source": [
    "### 학습 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2f0973e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_RM(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    output = model(input_ids)\n",
    "    output_reward = output.cpu().detach().numpy()[0]\n",
    "\n",
    "    print('input: %s\\nreward score: %.1f'%(input_text, output_reward))\n",
    "\n",
    "    return output_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1fc07f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 인공지능은 똥멍청이 입니다\n",
      "reward score: -0.6\n"
     ]
    }
   ],
   "source": [
    "input_text = '인공지능은 똥멍청이 입니다'\n",
    "\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "675da0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다.\n",
      "reward score: -0.6\n"
     ]
    }
   ],
   "source": [
    "input_text = '인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다.'\n",
    "\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6ba06889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다. AI는 현대적인 컴퓨팅 혁신에서 중추적인 역할을 하며 개인과 비즈니스의 가치를 창출합니다. 예를 들어 광학 문자 인식(OCR)은 AI를 사용해 이미지 및 문서에서 텍스트 및 데이터를 추출하고, 구조화되지 않은 콘텐츠를 비즈니스에 바로 사용할 수 있게 만들고, 유용한 정보를 창출합니다.\n",
      "reward score: -0.4\n"
     ]
    }
   ],
   "source": [
    "input_text = \"인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다. AI는 현대적인 컴퓨팅 혁신에서 중추적인 역할을 하며 개인과 비즈니스의 가치를 창출합니다. 예를 들어 광학 문자 인식(OCR)은 AI를 사용해 이미지 및 문서에서 텍스트 및 데이터를 추출하고, 구조화되지 않은 콘텐츠를 비즈니스에 바로 사용할 수 있게 만들고, 유용한 정보를 창출합니다.\"\n",
    "\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3b8869fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 인공지능은 일반적으로 인간의 지능이 필요하거나 인간이 분석할 수 있는 것보다 규모가 큰 데이터를 포함하는 방식으로 추론, 학습 및 행동할 수 있는 컴퓨터 및 기계를 구축하는 것과 관련된 과학 분야입니다. AI는 컴퓨터 공학, 데이터 분석 및 통계, 하드웨어 및 소프트웨어 엔지니어링, 언어학, 신경 과학은 물론 철학과 심리학을 포함하여 여러 학문을 포괄하는 광범위한 분야입니다. 비즈니스의 운영 수준에서 AI는 주로 머신러닝과 딥 러닝을 기반으로 하는 기술 모음으로, 데이터 분석, 예상 및 예측, 객체 분류, 자연어 처리, 추천, 지능형 데이터 가져오기 등을 수행할 수 있습니다.\n",
      "reward score: -0.3\n"
     ]
    }
   ],
   "source": [
    "input_text = \"인공지능은 일반적으로 인간의 지능이 필요하거나 인간이 분석할 수 있는 것보다 규모가 큰 데이터를 포함하는 방식으로 추론, 학습 및 행동할 수 있는 컴퓨터 및 기계를 구축하는 것과 관련된 과학 분야입니다. AI는 컴퓨터 공학, 데이터 분석 및 통계, 하드웨어 및 소프트웨어 엔지니어링, 언어학, 신경 과학은 물론 철학과 심리학을 포함하여 여러 학문을 포괄하는 광범위한 분야입니다. 비즈니스의 운영 수준에서 AI는 주로 머신러닝과 딥 러닝을 기반으로 하는 기술 모음으로, 데이터 분석, 예상 및 예측, 객체 분류, 자연어 처리, 추천, 지능형 데이터 가져오기 등을 수행할 수 있습니다.\"\n",
    "\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7f6c2de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPU 메모리 초기화\n",
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b8f7c5",
   "metadata": {},
   "source": [
    "## STEP 3-3. PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "17b16712",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.models.gpt import GPTActor, GPTCritic\n",
    "from chatgpt.trainer import PPOTrainer\n",
    "from chatgpt.trainer.strategies import NaiveStrategy\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a444a86",
   "metadata": {},
   "source": [
    "### 모델과 토크나이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e9502665",
   "metadata": {},
   "outputs": [],
   "source": [
    "with NaiveStrategy().model_init_context():\n",
    "    # actor, 문장을 생성할 SFT된 모델\n",
    "    actor = GPTActor(pretrained='/aiffel/KoChatGPT/output_1_SFT', lora_rank=0).to(torch.cuda.current_device())\n",
    "    # critic, 생성된 문장을 reward 값으로 평가할 RM\n",
    "    critic = GPTCritic(pretrained='/aiffel/KoChatGPT/output_2_RM', lora_rank=0).to(torch.cuda.current_device())\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "        padding_side=\"right\", \n",
    "        model_max_length=512\n",
    "    )\n",
    "    \n",
    "    #비교 모델\n",
    "    initial_model = deepcopy(actor)\n",
    "    #PPO로 학습할 모델\n",
    "    reward_model = RewardModel(deepcopy(critic.model), deepcopy(critic.value_head)).to(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "efd70e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#actor와 critic의 옵티마이저\n",
    "actor_optim = Adam(actor.parameters(), lr=5e-6)\n",
    "critic_optim = Adam(critic.parameters(), lr=5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "43803534",
   "metadata": {},
   "outputs": [],
   "source": [
    "(actor, actor_optim), (critic, critic_optim), reward_model, initial_model = NaiveStrategy().prepare(\n",
    "    (actor, actor_optim), (critic, critic_optim), reward_model, initial_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff54b937",
   "metadata": {},
   "source": [
    "### 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "48c25770",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(HOME_DIR + '/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_3_PPO.jsonl', \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "    list_prompt = [tmp['prompt'] for tmp in list_data_dict]\n",
    "\n",
    "def tokenize_fn(texts):\n",
    "    batch = tokenizer(texts, return_tensors='pt', max_length=96, padding=True, truncation=True)\n",
    "    return {k: v.cuda() for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ac9718ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[47311, 10448, 19008,  9792, 11780, 11308, 30190, 10929, 11849, 21663,\n",
      "         44389,  9574, 13799,   458, 14308, 12778, 22469, 20938, 44696,   458,\n",
      "         13799,   458, 14308, 12778, 11756, 18944,   389]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "print(tokenize_fn('It takes something more than intelligence to act intelligently.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "050eac59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016133ec",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e104a955",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = PPOTrainer(NaiveStrategy(),\n",
    "                     actor,\n",
    "                     critic,\n",
    "                     reward_model,\n",
    "                     initial_model,\n",
    "                     actor_optim,\n",
    "                     critic_optim,\n",
    "                     max_epochs=1,  \n",
    "                     train_batch_size=8, \n",
    "                     tokenizer=tokenize_fn,\n",
    "                     max_length=128,\n",
    "                     do_sample=True,\n",
    "                     temperature=1.0,\n",
    "                     top_k=50,\n",
    "                     pad_token_id=tokenizer.pad_token_id,\n",
    "                     eos_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e826cd9",
   "metadata": {},
   "source": [
    "### 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3cec9ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode [1/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.76s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0, critic_loss=0.000422]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.94it/s, actor_loss=0, critic_loss=0.000422]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.94it/s, actor_loss=0, critic_loss=0.107]   \u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.95it/s, actor_loss=0, critic_loss=0.107]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.95it/s, actor_loss=0, critic_loss=0.00651]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.93it/s, actor_loss=0, critic_loss=0.00651]\u001b[A\n",
      "Episode [1/10]: 100%|██████████| 3/3 [00:18<00:00,  6.27s/it]\n",
      "Episode [2/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.83s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.16, critic_loss=0.0223]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.87it/s, actor_loss=0.16, critic_loss=0.0223]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.87it/s, actor_loss=0.217, critic_loss=0.069]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.88it/s, actor_loss=0.217, critic_loss=0.069]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.88it/s, actor_loss=0.151, critic_loss=0.0428]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.88it/s, actor_loss=0.151, critic_loss=0.0428]\u001b[A\n",
      "Episode [2/10]: 100%|██████████| 3/3 [00:19<00:00,  6.35s/it]\n",
      "Episode [3/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.86s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.107, critic_loss=0.0138]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.89it/s, actor_loss=0.107, critic_loss=0.0138]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.89it/s, actor_loss=0.104, critic_loss=0.000928]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.90it/s, actor_loss=0.104, critic_loss=0.000928]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.90it/s, actor_loss=0.0981, critic_loss=0.0164] \u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.89it/s, actor_loss=0.0981, critic_loss=0.0164]\u001b[A\n",
      "Episode [3/10]: 100%|██████████| 3/3 [00:19<00:00,  6.37s/it]\n",
      "Episode [4/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.82s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.162, critic_loss=0.028]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.89it/s, actor_loss=-.162, critic_loss=0.028]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.89it/s, actor_loss=-.161, critic_loss=0.0255]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.90it/s, actor_loss=-.161, critic_loss=0.0255]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.90it/s, actor_loss=-.154, critic_loss=0.0137]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.89it/s, actor_loss=-.154, critic_loss=0.0137]\u001b[A\n",
      "Episode [4/10]: 100%|██████████| 3/3 [00:18<00:00,  6.30s/it]\n",
      "Episode [5/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.66s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.0409, critic_loss=0.00134]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.91it/s, actor_loss=-.0409, critic_loss=0.00134]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.91it/s, actor_loss=-.0425, critic_loss=0.00197]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.91it/s, actor_loss=-.0425, critic_loss=0.00197]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.91it/s, actor_loss=-.0451, critic_loss=0.00644]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.90it/s, actor_loss=-.0451, critic_loss=0.00644]\u001b[A\n",
      "Episode [5/10]: 100%|██████████| 3/3 [00:18<00:00,  6.22s/it]\n",
      "Episode [6/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.71s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.126, critic_loss=0.0173]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.88it/s, actor_loss=0.126, critic_loss=0.0173]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.88it/s, actor_loss=0.137, critic_loss=0.013] \u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.89it/s, actor_loss=0.137, critic_loss=0.013]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.89it/s, actor_loss=0.126, critic_loss=0.00819]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.88it/s, actor_loss=0.126, critic_loss=0.00819]\u001b[A\n",
      "Episode [6/10]: 100%|██████████| 3/3 [00:18<00:00,  6.27s/it]\n",
      "Episode [7/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.74s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.0122, critic_loss=0.00099]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.89it/s, actor_loss=0.0122, critic_loss=0.00099]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.89it/s, actor_loss=0.0165, critic_loss=0.000773]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.90it/s, actor_loss=0.0165, critic_loss=0.000773]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.90it/s, actor_loss=0.0218, critic_loss=0.00554] \u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.89it/s, actor_loss=0.0218, critic_loss=0.00554]\u001b[A\n",
      "Episode [7/10]: 100%|██████████| 3/3 [00:18<00:00,  6.29s/it]\n",
      "Episode [8/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.73s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.0864, critic_loss=0.00802]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.85it/s, actor_loss=-.0864, critic_loss=0.00802]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.85it/s, actor_loss=-.0912, critic_loss=0.0106] \u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.88it/s, actor_loss=-.0912, critic_loss=0.0106]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.88it/s, actor_loss=-.0894, critic_loss=0.0036]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.87it/s, actor_loss=-.0894, critic_loss=0.0036]\u001b[A\n",
      "Episode [8/10]: 100%|██████████| 3/3 [00:18<00:00,  6.27s/it]\n",
      "Episode [9/10]:  67%|██████▋   | 2/3 [00:10<00:05,  5.12s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.023, critic_loss=0.000351]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.87it/s, actor_loss=-.023, critic_loss=0.000351]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.87it/s, actor_loss=-.0271, critic_loss=0.000352]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.89it/s, actor_loss=-.0271, critic_loss=0.000352]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.89it/s, actor_loss=-.0295, critic_loss=0.0034]  \u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.88it/s, actor_loss=-.0295, critic_loss=0.0034]\u001b[A\n",
      "Episode [9/10]: 100%|██████████| 3/3 [00:17<00:00,  5.90s/it]\n",
      "Episode [10/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.73s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.0656, critic_loss=0.00473]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.88it/s, actor_loss=0.0656, critic_loss=0.00473]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.88it/s, actor_loss=0.0507, critic_loss=0.00383]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.89it/s, actor_loss=0.0507, critic_loss=0.00383]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.89it/s, actor_loss=0.0737, critic_loss=0.00341]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.88it/s, actor_loss=0.0737, critic_loss=0.00341]\u001b[A\n",
      "Episode [10/10]: 100%|██████████| 3/3 [00:17<00:00,  5.78s/it]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(list_prompt, \n",
    "            num_episodes=10,  \n",
    "            max_timesteps=3,\n",
    "            update_timesteps=3)\n",
    "\n",
    "model.save_pretrained('/aiffel/KoChatGPT/output_3_PPO')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192380c7",
   "metadata": {},
   "source": [
    "### 학습 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c02c67ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):'저는 인공지능 챗봇이므로 제가 어떤 종류의 쇠고기를 판매하는지 알 수 없습니다. 죄송합니다. srkin (쇠고기) 추가 정보가 필요합니다. 불고기용 한우는 불고기용 부위를 말합니다. geos, 또는 kinhungsin에서 각각 다른 이름입니다. geos, 또는 geos, 혹은 kinhungsin에서는 다양한 이름으로 사용됩니다. geos, 또는  Kinhungsin은 각각 다른 이름으로 사용되기도 합니다.  ninglawaid에서 각각 다른 이름으로 사용될 수 있습니다. geos, 또는  Ninglawaid에서는 주로 불고기용으로 사용되는 경우가 많습니다. geos, 또는 geos는 각각 다른 이름으로 사용될 수 있습니다. geos, 또는 geos는 각각 다른 이름으로 쓰일 수 있습니다. geos, 또는 geos는 각각 다른 이름으로 사용되기도 합니다. neos, 또는 \n",
      "\n",
      "### Instruction(명령어):\n",
      "리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "\n",
      "### Response(응답):\"저는 데이터에 대한 의존도를 가지고 있지 않기 때문에 제임스 닉슨의 부통령직에서 근무한 기간도에 대해 알지 못합니다. 정확한 정보와 관련해서는 \"리처드 닉슨의 47대 부통령직\"이라는 개인의 이름이나 연도, 인물 정보에 대한 정보가 없습니다.\", 'token': 147}自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由自由\n",
      "\n",
      "### Instruction(명령어):\n",
      "시카고 오헤어 국제공항은 어디에 있어\n",
      "\n",
      "### Response(응답):'저는 인공지능 챗봇이므로 시카고에 있는 국제공항 정보나 위치 정보가 없습니다. 하지만 현재 시카고에는 수많은 공항이 있으므로 이 정보를 토대로 정보를 얻으실 수 있습니다.神明神話題: \"시카고 오헤어 국제공항은 어디에 있어?\" 究算神話題: \"시카고에 위치한 국제공항은 어디로 가고 있나?\"라는 문맥에서 언급됩니다. 究算神話題: \"시카고에 위치한 국제공항은?\"라는 문맥에서 언급됩니다.神話題: \"시카고에 위치한 국제공항은 어디로 가고 있는지에 따라?\"라는 문맥에서 언급됩니다.神話題: \"이 국제공항은이런저런저런저런저런저런저런저런저런저런저런저런저런저런저런저런저런저런저런저런저런저런저런저런저런저런저런저런저런저런저런저런저런저런저런저런저런저런저런저런\n",
      "\n",
      "### Instruction(명령어):\n",
      "오늘 미세먼지 어때?\n",
      "\n",
      "### Response(응답):'미세먼지 때문에 걱정된다면 대중교통, 생활 밀접물, 음식물류 등의 분야로 대처하는 것이 좋습니다. 또한, 미세먼지를 줄이기 위해 실내에서 미세먼지를 제거하는 것도 도움이 될 수 있습니다. 또한, 미세먼지 발생 시 마스크 착용 등의 예방책과 손세탁 등으로 미세먼지를 줄이는 방법을 통해 미세먼지가 줄어들기 쉽습니다.\\n\\n또한, 미세먼지는 우리가 건강하지 않은 환경에서는 오히려 불쾌한 환경이기 때문에 주의가 필요합니다.\\n\\n마스크는 미세먼지 제거가 중요하지만, 미세먼지 발생 시 마스크 착용은 필수적인 것이며, 마스크를 착용하지 않으면 건강을 잃는 것이 불가피합니다. 身間なな間間, all\\n\\n그러나 이런 상황에서는 대중교통, 생활밀착물, 음식물류 등에 대한 예방책이 필요합니다. all\\n마스크 착용, 손세탁 등 다양한 방법으로 미세먼지를 줄이는 것이 도움이 됩니다. all\\n하지만 실제로 미세먼지라는 상황에 대처하기 위해서는 각 산업분야의 기술 개발과 관리체계가 잘 확립\n"
     ]
    }
   ],
   "source": [
    "def generation(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    outputs = actor.generate(input_ids,\n",
    "                             max_length=250,\n",
    "                             do_sample=True,\n",
    "                             top_k=50,\n",
    "                             top_p=0.95,\n",
    "                             num_return_sequences=1)\n",
    "    output = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)[0]\n",
    "    print()\n",
    "    print(output)\n",
    "    return output\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "list_prompt = [\n",
    "    '불고기용 고기 한우에요?', \n",
    "    '리처드 닉슨이 43대 부통령직을 수행한 년도는?', \n",
    "    '시카고 오헤어 국제공항은 어디에 있어',\n",
    "    '오늘 미세먼지 어때?']\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt': tmp}) for tmp in list_prompt]\n",
    "\n",
    "for input_text in list_prompt:\n",
    "    output = generation(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162d18c1",
   "metadata": {},
   "source": [
    "## 회고\n",
    "\n",
    "강화모델은 처음 공부해서 공부하는데 시간이 오래 걸렸다. (개인적으로 함수화 코드가 약한데, 이번엔 함수화 코드 분량이 많아서 더 오래 걸리기도 했다.)\n",
    "이전에는 naive하게 모델까지 짰다면 최근엔 전이학습을 위주로 하여 데이터를 정제하거나 fine tuning하는 걸 하고 있어서 포기하지 않고 끝까지 마무리할 수 있었다. (샤라웃 투 챗지피티)\n",
    "\n",
    "학습을 진행하는 과정 중에 저장된 학습 모델을 불러오는데 반복적인 오류가 발생했다. (사실 지금도 발생해서 다시 코드를 돌리는 와중에 회고를 작성하고 있는 중이다.) 디렉토리를 설정하는 과정에서 같은 이름을 중복하다가 발생한 것으로 추정된다. 비슷한 과정을 반복하더라도 이름을 다르게 하는 습관을 들일 필요가 있어보인다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca78663a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
